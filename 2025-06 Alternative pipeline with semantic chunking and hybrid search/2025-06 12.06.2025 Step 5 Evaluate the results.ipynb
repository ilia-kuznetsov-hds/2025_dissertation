{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f6088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7ad1ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG = r\"C:\\\\Users\\\\kuzne\\\\Documents\\\\Python_repo\\\\2025_01_dissertation\\\\2025_dissertation\\\\data\\\\2025-06 hybrid search\\\\psychiatry_train_dataset_groq_mistral-saba-24b_answered_rag_answer_correctness_evaluated.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c57fda73",
   "metadata": {},
   "outputs": [],
   "source": [
    "VANILLA = r\"C:\\\\Users\\\\kuzne\\\\Documents\\\\Python_repo\\\\2025_01_dissertation\\\\2025_dissertation\\\\data\\\\2025-06 hybrid search\\\\psychiatry_train_dataset_groq_mistral-saba-24b_answered_vanilla_answer_correctness_evaluated.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06f288f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Mean Answer Correctness: 0.4384988387275732\n",
      "Vanilla Mean Answer Correctness: 0.4564869899367323\n"
     ]
    }
   ],
   "source": [
    "# Read the files\n",
    "rag_df = pd.read_csv(RAG)  # Replace with actual filename\n",
    "vanilla_df = pd.read_csv(VANILLA)  # Replace with actual filename\n",
    "\n",
    "# Calculate means\n",
    "rag_mean = rag_df['Mean Answer Correctness for RAG'].mean()\n",
    "vanilla_mean = vanilla_df['Mean Answer Correctness for vanilla'].mean()\n",
    "\n",
    "print(f\"RAG Mean Answer Correctness: {rag_mean}\")\n",
    "print(f\"Vanilla Mean Answer Correctness: {vanilla_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b737f",
   "metadata": {},
   "source": [
    "The very first results were:  \n",
    "RAG Mean Answer Correctness: 0.4123253574398246  \n",
    "Vanilla Mean Answer Correctness: 0.4220597875819323  \n",
    "\n",
    "Addition of Naive RAG deteriorates the answer quality. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef69b621",
   "metadata": {},
   "source": [
    "Hybrid Search pipeline:  \n",
    "RAG Mean Answer Correctness: 0.4384988387275732  \n",
    "Vanilla Mean Answer Correctness: 0.4564869899367323"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e242e8cb",
   "metadata": {},
   "source": [
    "Llama 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5548b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_3B_VANILLA = r\"C:\\\\Users\\\\kuzne\\\\Documents\\\\Python_repo\\\\2025_01_dissertation\\\\2025_dissertation\\\\data\\\\2025-06 hybrid search\\\\psychiatry_test_dataset_together_meta-llama_Llama-3.2-3B-Instruct-Turbo_answered_vanilla_answer_correctness_evaluated.csv\"\n",
    "\n",
    "LLAMA_3B_RAG = r\"C:\\\\Users\\\\kuzne\\\\Documents\\\\Python_repo\\\\2025_01_dissertation\\\\2025_dissertation\\\\data\\\\2025-06 hybrid search\\\\psychiatry_test_dataset_together_meta-llama_Llama-3.2-3B-Instruct-Turbo_answered_rag_answer_correctness_evaluated.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc185500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Mean Answer Correctness: 0.40866393402516055\n",
      "Vanilla Mean Answer Correctness: 0.4160153709655587\n"
     ]
    }
   ],
   "source": [
    "# Read the files\n",
    "rag_df = pd.read_csv(LLAMA_3B_RAG)  # Replace with actual filename\n",
    "vanilla_df = pd.read_csv(LLAMA_3B_VANILLA)  # Replace with actual filename\n",
    "\n",
    "# Calculate means\n",
    "llama3b_rag_mean = rag_df['Mean Answer Correctness for RAG'].mean()\n",
    "llama3b_vanilla_mean = vanilla_df['Mean Answer Correctness for vanilla'].mean()\n",
    "\n",
    "print(f\"RAG Mean Answer Correctness: {llama3b_rag_mean}\")\n",
    "print(f\"Vanilla Mean Answer Correctness: {llama3b_vanilla_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8042369",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE = r\"C:\\\\Users\\\\kuzne\\\\Documents\\\\Python_repo\\\\2025_01_dissertation\\\\2025_dissertation\\\\data\\\\2025-06 hybrid search\\\\psychiatry_test_dataset_together_meta-llama_Llama-3.2-3B-Instruct-Turbo_answeredsimply_evaluated.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0a54dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Mean Answer Correctness: 0.5270880361173815\n",
      "Simple Mean Answer Correctness for RAG: 0.4345372460496614\n"
     ]
    }
   ],
   "source": [
    "simple = pd.read_csv(SIMPLE)  # Replace with actual filename\n",
    "\n",
    "# Calculate means\n",
    "\n",
    "simple_mean_vanilla = simple['GPT-based scoring for vanilla'].mean()\n",
    "print(f\"Simple Mean Answer Correctness: {simple_mean_vanilla}\")\n",
    "\n",
    "simple_mean_rag = simple['GPT-based scoring for RAG'].mean()\n",
    "print(f\"Simple Mean Answer Correctness for RAG: {simple_mean_rag}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
