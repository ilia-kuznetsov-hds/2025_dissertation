{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e712d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135038a7",
   "metadata": {},
   "source": [
    "Evaluation setup:  \n",
    "* Embedding model - Text Embedding 004 (Google)  \n",
    "* LLM - Gemini 2.5 Flash  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd1067",
   "metadata": {},
   "source": [
    "## RAGAS Answer Correctness:\n",
    "\n",
    "The assessment of Answer Correctness involves measuring the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness.\n",
    "Answer correctness  is computed as the sum of factual correctness and the semantic similarity between the given answer and the ground truth.  \n",
    "\n",
    "Factual correctness is a metric that compares and evaluates the factual accuracy of the generated response with the reference. This metric is used to determine the extent to which the generated response aligns with the reference. The factual correctness score ranges from 0 to 1, with higher values indicating better performance. To measure the alignment between the response and the reference, the metric uses the LLM to first break down the response and reference into claims and then uses natural language inference to determine the factual overlap between the response and the reference. Factual overlap is quantified using precision, recall, and F1 score, which can be controlled using the mode parameter. By default, the mode is set to F1, you can change the mode to precision or recall by setting the mode parameter.\n",
    "\n",
    "Answer similarity is calculated by following steps:  \n",
    "Step 1: Vectorize the ground truth answer using the embedding model.  \n",
    "Step 2: Vectorize the generated answer using the same embedding model.  \n",
    "Step 3: Compute the cosine similarity between the two vectors.  \n",
    "        \n",
    "By default \"text-embedding-ada-002\" model is used. In that evaluation, we used Text Embedding 004 (Google).  \n",
    "\n",
    "Final score is created by taking a weighted average of the factual correctness (F1 score) and the semantic similarity. \n",
    "(By default, there is a 0.75 : 0.25 weighting.)   \n",
    "\n",
    "Total API Calls: 4  \n",
    "* 1 LLM call to produce the \"simple statements  \n",
    "* 1 LLM call to determine the true positives, false positives, and false negatives  \n",
    "* 1 embedding call to embed the context  \n",
    "* 1 embedding call to embed the AI answer  \n",
    "\n",
    "I feel like RAGAS use much more in terms of LLM calls. This is a source code:  \n",
    "https://github.com/explodinggradients/ragas/blob/main/ragas/src/ragas/metrics/_answer_correctness.py\n",
    "\n",
    "There is a compaint about RAGAS costing 700$ for 100 QA set:  \n",
    "https://www.reddit.com/r/LangChain/comments/1dbmqii/i_spent_700_on_evaluating_100_rag_qa_set_using/\n",
    "\n",
    "Because the generated results is non-deterministic, I run evaluation 3 times and use mean number. 370 questions x 3 times x 2 (vanilla and RAG) X 4 API calls = 8880 API calls total for one set of results.\n",
    "When I decided to play with Gemini 2.5 flash, â€‹I also got results for another metrics, in total dashboard shows 16000 API calls and 167 dollars spent.  \n",
    "Conclusion: I will stick with Gemini 2.0 flash model as main judge for the rest of the project\n",
    "\n",
    "Source:  \n",
    "https://docs.ragas.io/en/v0.1.21/concepts/metrics/answer_correctness.html  \n",
    "https://github.com/dkhundley/llm-rag-guide/blob/main/notebooks/ragas.ipynb  \n",
    "https://docs.ragas.io/en/stable/references/embeddings/#ragas.embeddings.embedding_factory  \n",
    "https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/factual_correctness/#factual-correctness  \n",
    "https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/semantic_similarity/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48695377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean Answer Correctness for Vanilla: 0.6058\n",
      "Overall Mean Answer Correctness for RAG: 0.5695\n",
      "Difference (RAG - Vanilla): -0.0364\n"
     ]
    }
   ],
   "source": [
    "file_name = \"test_dataset_together_meta-llama_Llama-4-Scout-17B-16E-Instruct_top5_answered.json\"\n",
    "base_name = file_name.replace('.json', '')\n",
    "\n",
    "    # Use forward slashes or os.path.join for better cross-platform compatibility\n",
    "VANILLA_ANSWER_CORRECTNESS = f\"{base_name}_vanilla_answer_correctness_evaluated.json\"\n",
    "RAG_ANSWER_CORRECTNESS = f\"{base_name}_rag_answer_correctness_evaluated.json\"\n",
    "RAG_ANSWER_SIMILARITY = f\"{base_name}_rag_answer_similarity_evaluated.json\"\n",
    "VANILLA_ANSWER_SIMILARITY = f\"{base_name}_vanilla_answer_similarity_evaluated.json\"\n",
    "RAG_ANSWER_RELEVANCY = f\"{base_name}_rag_answer_relevancy_evaluated.json\"\n",
    "RAG_FAITHFULNESS = f\"{base_name}_rag_faithfulness_evaluated.json\"\n",
    "\n",
    "with open(VANILLA_ANSWER_CORRECTNESS, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)   \n",
    "df_1 = pd.DataFrame(data)\n",
    "\n",
    "with open(RAG_ANSWER_CORRECTNESS, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f) \n",
    "df_2 = pd.DataFrame(data)\n",
    "df_2 = df_2[[\"Modified Questions\", \n",
    "                 \"Answer Correctness for RAG run 1\", \n",
    "                 \"Answer Correctness for RAG run 2\",\n",
    "                 \"Answer Correctness for RAG run 3\",\n",
    "                 \"Mean Answer Correctness for RAG\"]]\n",
    "    \n",
    "merged_df = pd.merge(df_1, df_2, on=\"Modified Questions\", how=\"inner\")\n",
    "# Calculate overall means\n",
    "vanilla_mean = merged_df['Mean Answer Correctness for vanilla'].mean()\n",
    "rag_mean = merged_df['Mean Answer Correctness for RAG'].mean()\n",
    "    \n",
    "print(f\"Overall Mean Answer Correctness for Vanilla: {vanilla_mean:.4f}\")\n",
    "print(f\"Overall Mean Answer Correctness for RAG: {rag_mean:.4f}\")\n",
    "print(f\"Difference (RAG - Vanilla): {rag_mean - vanilla_mean:.4f}\")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f77719a",
   "metadata": {},
   "source": [
    "Again, vanilla LLM response is evaluated higher than RAG-enhanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b0eef0",
   "metadata": {},
   "source": [
    "## RAGAS Answer semantic similarity  \n",
    "\n",
    "This evaluation is based on the ground truth and the answer, with values falling within the range of 0 to 1. A higher score signifies a better alignment \n",
    "between the generated answer and the ground truth.  \n",
    "Step 1: Vectorize the ground truth answer using the specified embedding model.  \n",
    "Step 2: Vectorize the generated answer using the same embedding model.  \n",
    "Step 3: Compute the cosine similarity between the two vectors.  \n",
    "\n",
    "The metric is a part of RAGAS Answer Correctness metric. The Answer Correctness's final score is created by taking a weighted average of the factual correctness (F1 score) and the semantic similarity. \n",
    "(By default, there is a 0.75 : 0.25 weighting.)   \n",
    "\n",
    "\n",
    "Total API Calls: 2\n",
    "* 1 embedding call to embed the ground truth\n",
    "* 1 embedding call to embed the AI answer  \n",
    "\n",
    "The metric is produced much faster (around 30 minutes for the whole set of results). It is 25% component of Answer Correctness metric.   \n",
    "\n",
    "Source:  \n",
    "https://docs.ragas.io/en/v0.1.21/concepts/metrics/semantic_similarity.html\n",
    "https://github.com/dkhundley/llm-rag-guide/blob/main/notebooks/ragas.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32659d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean Answer Semantic Similarity for RAG: 0.8752\n",
      "Overall Mean Answer Semantic Similarity for Vanilla: 0.8794\n",
      "Difference (RAG - Vanilla): -0.0042\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vanilla_Mean</th>\n",
       "      <th>RAG_Mean</th>\n",
       "      <th>Difference (RAG - Vanilla)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychiatric_category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Eating Disorders</th>\n",
       "      <td>0.8511</td>\n",
       "      <td>0.8656</td>\n",
       "      <td>0.0145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Somatic Disorders</th>\n",
       "      <td>0.8635</td>\n",
       "      <td>0.8762</td>\n",
       "      <td>0.0127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality Disorders</th>\n",
       "      <td>0.8951</td>\n",
       "      <td>0.9062</td>\n",
       "      <td>0.0111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anxiety Disorders</th>\n",
       "      <td>0.8841</td>\n",
       "      <td>0.8850</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Mental Disorders</th>\n",
       "      <td>0.8745</td>\n",
       "      <td>0.8709</td>\n",
       "      <td>-0.0036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dissociative Disorders</th>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.9323</td>\n",
       "      <td>-0.0041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bipolar Disorders</th>\n",
       "      <td>0.8830</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>-0.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trauma and Stressor Related Disorders</th>\n",
       "      <td>0.8932</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>-0.0083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Schizophrenia Spectrum and Other Psychotic Disorders</th>\n",
       "      <td>0.8835</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>-0.0083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depressive Disorders</th>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.8630</td>\n",
       "      <td>-0.0105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obsessive-Compulsive Disorders</th>\n",
       "      <td>0.9039</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>-0.0228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Vanilla_Mean  RAG_Mean  \\\n",
       "psychiatric_category                                                         \n",
       "Eating Disorders                                          0.8511    0.8656   \n",
       "Somatic Disorders                                         0.8635    0.8762   \n",
       "Personality Disorders                                     0.8951    0.9062   \n",
       "Anxiety Disorders                                         0.8841    0.8850   \n",
       "Other Mental Disorders                                    0.8745    0.8709   \n",
       "Dissociative Disorders                                    0.9364    0.9323   \n",
       "Bipolar Disorders                                         0.8830    0.8783   \n",
       "Trauma and Stressor Related Disorders                     0.8932    0.8849   \n",
       "Schizophrenia Spectrum and Other Psychotic Diso...        0.8835    0.8752   \n",
       "Depressive Disorders                                      0.8735    0.8630   \n",
       "Obsessive-Compulsive Disorders                            0.9039    0.8811   \n",
       "\n",
       "                                                    Difference (RAG - Vanilla)  \n",
       "psychiatric_category                                                            \n",
       "Eating Disorders                                                        0.0145  \n",
       "Somatic Disorders                                                       0.0127  \n",
       "Personality Disorders                                                   0.0111  \n",
       "Anxiety Disorders                                                       0.0009  \n",
       "Other Mental Disorders                                                 -0.0036  \n",
       "Dissociative Disorders                                                 -0.0041  \n",
       "Bipolar Disorders                                                      -0.0047  \n",
       "Trauma and Stressor Related Disorders                                  -0.0083  \n",
       "Schizophrenia Spectrum and Other Psychotic Diso...                     -0.0083  \n",
       "Depressive Disorders                                                   -0.0105  \n",
       "Obsessive-Compulsive Disorders                                         -0.0228  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with open(RAG_ANSWER_SIMILARITY, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f) \n",
    "df_3 = pd.DataFrame(data)\n",
    "df_3 = df_3[[\"Modified Questions\", \n",
    "            \"Answer Semantic Similarity for rag\"]]\n",
    "\n",
    "\n",
    "with open(VANILLA_ANSWER_SIMILARITY, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f) \n",
    "df_4 = pd.DataFrame(data)\n",
    "df_4 = df_4[[\"Modified Questions\", \n",
    "            \"Answer Semantic Similarity for vanilla\"]]\n",
    "\n",
    "merged_df = pd.merge(merged_df, df_3, on=\"Modified Questions\", how=\"inner\")\n",
    "merged_df = pd.merge(merged_df, df_4, on=\"Modified Questions\", how=\"inner\")\n",
    "\n",
    "# Calculate overall means\n",
    "rag_similarity_mean = merged_df['Answer Semantic Similarity for rag'].mean()\n",
    "vanilla_similarity_mean = merged_df['Answer Semantic Similarity for vanilla'].mean()\n",
    "print(f\"Overall Mean Answer Semantic Similarity for RAG: {rag_similarity_mean:.4f}\")\n",
    "print(f\"Overall Mean Answer Semantic Similarity for Vanilla: {vanilla_similarity_mean:.4f}\")\n",
    "print(f\"Difference (RAG - Vanilla): {rag_similarity_mean - vanilla_similarity_mean:.4f}\")\n",
    "\n",
    "# Group by psychiatric category\n",
    "category_stats = merged_df.groupby('psychiatric_category').agg({\n",
    "        'Answer Semantic Similarity for vanilla': 'mean',\n",
    "        'Answer Semantic Similarity for rag': 'mean'}).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "category_stats.columns = ['Vanilla_Mean', 'RAG_Mean']\n",
    "    \n",
    "    # Add difference column\n",
    "category_stats['Difference (RAG - Vanilla)'] = (category_stats['RAG_Mean'] - \n",
    "                                                   category_stats['Vanilla_Mean']).round(4)\n",
    "    \n",
    "    # Sort by difference to see which categories benefit most from RAG\n",
    "category_stats = category_stats.sort_values('Difference (RAG - Vanilla)', ascending=False)\n",
    "    \n",
    "display(category_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1612bd7",
   "metadata": {},
   "source": [
    "# RAG Metrics  \n",
    "## RAGAS Faithfulness \n",
    "Faithfulness measures the factual consistency of the generated answer against the given context. It is calculated from the answer and the retrieved context. The answer is scaled to the (0, 1) range. The higher the better. \n",
    "The generated answer is regarded as faithful if all the claims made in the answer can be inferred from the given context.  \n",
    "At the first step, the generated answer is broken down into individual statements.     \n",
    "Then each of these claims is cross-checked with the given context to determine if it can be inferred from the context.    \n",
    "The final score is calculated by dividing the number of claims that can be inferred from the context by the total number of claims in the generated response.  \n",
    "\n",
    "\n",
    "https://docs.ragas.io/en/v0.1.21/concepts/metrics/faithfulness.html\n",
    "\n",
    "\n",
    "Groundedness, sometimes referred to as faithfulness, measures whether the response is based completely on the context. It validates that the response isn't using information other than what exists in the context. A low groundedness metric indicates that the language model might be outputting inaccurate or nonsensical responses.  A low groundedness calculation indicates that the language model doesn't see the chunks as relevant. You should evaluate whether you need to add data to your collection, adjust your chunking strategy or chunk size, or fine-tune your prompt.  \n",
    "https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-llm-evaluation-phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a435f6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean Faithfulness for RAG: 0.2678\n"
     ]
    }
   ],
   "source": [
    "with open(RAG_FAITHFULNESS, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f) \n",
    "df_3 = pd.DataFrame(data)\n",
    "df_3 = df_3[[\"Modified Questions\", \n",
    "                \"faithfulness for RAG run 1\",\n",
    "                \"faithfulness for RAG run 2\",\n",
    "                \"faithfulness for RAG run 3\",\n",
    "                \"Mean faithfulness for RAG\"]]\n",
    "    \n",
    "merged_df = pd.merge(merged_df, df_3, on=\"Modified Questions\", how=\"inner\")\n",
    "\n",
    "faithfulness_mean = merged_df['Mean faithfulness for RAG'].mean()\n",
    "print(f\"Overall Mean Faithfulness for RAG: {faithfulness_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f09b26",
   "metadata": {},
   "source": [
    "Only 26% of retrieved statements are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9cff34",
   "metadata": {},
   "source": [
    "## RAGAS Answer Relevancy\n",
    "\n",
    "\n",
    "Step 1: Reverse-engineer â€˜nâ€™ variants of the question from the generated answer using a Large Language Model (LLM)  \n",
    "Step 2: Calculate the mean cosine similarity between the generated questions and the actual question.\n",
    "\n",
    "\n",
    "\n",
    "https://docs.ragas.io/en/v0.1.21/concepts/metrics/answer_relevance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3916febd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean Relevancy for RAG: 0.7071\n",
      "                                                    RAG_Mean\n",
      "psychiatric_category                                        \n",
      "Dissociative Disorders                                0.8022\n",
      "Anxiety Disorders                                     0.7822\n",
      "Eating Disorders                                      0.7656\n",
      "Obsessive-Compulsive Disorders                        0.7543\n",
      "Depressive Disorders                                  0.7442\n",
      "Personality Disorders                                 0.7439\n",
      "Trauma and Stressor Related Disorders                 0.7069\n",
      "Schizophrenia Spectrum and Other Psychotic Diso...    0.7028\n",
      "Somatic Disorders                                     0.6944\n",
      "Bipolar Disorders                                     0.6814\n",
      "Other Mental Disorders                                0.6642\n"
     ]
    }
   ],
   "source": [
    "with open(RAG_ANSWER_RELEVANCY, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f) \n",
    "\n",
    "df_4 = pd.DataFrame(data)\n",
    "df_4 = df_4[[\"Modified Questions\",\n",
    "            \"answer_relevancy for RAG run 1\",\n",
    "    \"answer_relevancy for RAG run 2\",\n",
    "    \"answer_relevancy for RAG run 3\",\n",
    "    \"Mean answer_relevancy for RAG\"]]\n",
    "merged_df = pd.merge(merged_df, df_4, on=\"Modified Questions\", how=\"inner\")\n",
    "\n",
    "relevancy_mean = merged_df['Mean answer_relevancy for RAG'].mean()\n",
    "print(f\"Overall Mean Relevancy for RAG: {relevancy_mean:.4f}\")\n",
    "\n",
    "# Group by psychiatric category\n",
    "category_stats = merged_df.groupby('psychiatric_category').agg({\n",
    "        'Mean answer_relevancy for RAG': 'mean'}).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "category_stats.columns = ['RAG_Mean']\n",
    "       \n",
    "# Sort by difference to see which categories benefit most from RAG\n",
    "category_stats = category_stats.sort_values('RAG_Mean', ascending=False)\n",
    "    \n",
    "print(category_stats)\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcab57c",
   "metadata": {},
   "source": [
    "## Final function to generate report \n",
    "(work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e703c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Evaluation of Llama-4-Scout Results:\n",
      "\n",
      "Answer Semantic Similarity Results:\n",
      "Overall Mean Answer Semantic Similarity for RAG: 0.8752\n",
      "Overall Mean Answer Semantic Similarity for Vanilla: 0.8794\n",
      "Difference (RAG - Vanilla): -0.0042\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vanilla_Mean</th>\n",
       "      <th>RAG_Mean</th>\n",
       "      <th>Difference (RAG - Vanilla)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychiatric_category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Eating Disorders</th>\n",
       "      <td>0.8511</td>\n",
       "      <td>0.8656</td>\n",
       "      <td>0.0145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Somatic Disorders</th>\n",
       "      <td>0.8635</td>\n",
       "      <td>0.8762</td>\n",
       "      <td>0.0127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality Disorders</th>\n",
       "      <td>0.8951</td>\n",
       "      <td>0.9062</td>\n",
       "      <td>0.0111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anxiety Disorders</th>\n",
       "      <td>0.8841</td>\n",
       "      <td>0.8850</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Mental Disorders</th>\n",
       "      <td>0.8745</td>\n",
       "      <td>0.8709</td>\n",
       "      <td>-0.0036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dissociative Disorders</th>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.9323</td>\n",
       "      <td>-0.0041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bipolar Disorders</th>\n",
       "      <td>0.8830</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>-0.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trauma and Stressor Related Disorders</th>\n",
       "      <td>0.8932</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>-0.0083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Schizophrenia Spectrum and Other Psychotic Disorders</th>\n",
       "      <td>0.8835</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>-0.0083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depressive Disorders</th>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.8630</td>\n",
       "      <td>-0.0105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obsessive-Compulsive Disorders</th>\n",
       "      <td>0.9039</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>-0.0228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Vanilla_Mean  RAG_Mean  \\\n",
       "psychiatric_category                                                         \n",
       "Eating Disorders                                          0.8511    0.8656   \n",
       "Somatic Disorders                                         0.8635    0.8762   \n",
       "Personality Disorders                                     0.8951    0.9062   \n",
       "Anxiety Disorders                                         0.8841    0.8850   \n",
       "Other Mental Disorders                                    0.8745    0.8709   \n",
       "Dissociative Disorders                                    0.9364    0.9323   \n",
       "Bipolar Disorders                                         0.8830    0.8783   \n",
       "Trauma and Stressor Related Disorders                     0.8932    0.8849   \n",
       "Schizophrenia Spectrum and Other Psychotic Diso...        0.8835    0.8752   \n",
       "Depressive Disorders                                      0.8735    0.8630   \n",
       "Obsessive-Compulsive Disorders                            0.9039    0.8811   \n",
       "\n",
       "                                                    Difference (RAG - Vanilla)  \n",
       "psychiatric_category                                                            \n",
       "Eating Disorders                                                        0.0145  \n",
       "Somatic Disorders                                                       0.0127  \n",
       "Personality Disorders                                                   0.0111  \n",
       "Anxiety Disorders                                                       0.0009  \n",
       "Other Mental Disorders                                                 -0.0036  \n",
       "Dissociative Disorders                                                 -0.0041  \n",
       "Bipolar Disorders                                                      -0.0047  \n",
       "Trauma and Stressor Related Disorders                                  -0.0083  \n",
       "Schizophrenia Spectrum and Other Psychotic Diso...                     -0.0083  \n",
       "Depressive Disorders                                                   -0.0105  \n",
       "Obsessive-Compulsive Disorders                                         -0.0228  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "RAG Triad - Faithfulness Results:\n",
      "Overall Mean Faithfulness for RAG: 0.2678\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean faithfulness for RAG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychiatric_category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bipolar Disorders</th>\n",
       "      <td>0.3454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depressive Disorders</th>\n",
       "      <td>0.3222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obsessive-Compulsive Disorders</th>\n",
       "      <td>0.3119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anxiety Disorders</th>\n",
       "      <td>0.3085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Schizophrenia Spectrum and Other Psychotic Disorders</th>\n",
       "      <td>0.2932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eating Disorders</th>\n",
       "      <td>0.2464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Mental Disorders</th>\n",
       "      <td>0.2375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality Disorders</th>\n",
       "      <td>0.2359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trauma and Stressor Related Disorders</th>\n",
       "      <td>0.1550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Somatic Disorders</th>\n",
       "      <td>0.1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dissociative Disorders</th>\n",
       "      <td>0.1190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Mean faithfulness for RAG\n",
       "psychiatric_category                                                         \n",
       "Bipolar Disorders                                                      0.3454\n",
       "Depressive Disorders                                                   0.3222\n",
       "Obsessive-Compulsive Disorders                                         0.3119\n",
       "Anxiety Disorders                                                      0.3085\n",
       "Schizophrenia Spectrum and Other Psychotic Diso...                     0.2932\n",
       "Eating Disorders                                                       0.2464\n",
       "Other Mental Disorders                                                 0.2375\n",
       "Personality Disorders                                                  0.2359\n",
       "Trauma and Stressor Related Disorders                                  0.1550\n",
       "Somatic Disorders                                                      0.1253\n",
       "Dissociative Disorders                                                 0.1190"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "RAG Triad - Answer Relevancy Results:\n",
      "Overall Mean Relevancy for RAG: 0.7071\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean answer_relevancy for RAG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychiatric_category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dissociative Disorders</th>\n",
       "      <td>0.8022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anxiety Disorders</th>\n",
       "      <td>0.7822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eating Disorders</th>\n",
       "      <td>0.7656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obsessive-Compulsive Disorders</th>\n",
       "      <td>0.7543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depressive Disorders</th>\n",
       "      <td>0.7442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality Disorders</th>\n",
       "      <td>0.7439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trauma and Stressor Related Disorders</th>\n",
       "      <td>0.7069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Schizophrenia Spectrum and Other Psychotic Disorders</th>\n",
       "      <td>0.7028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Somatic Disorders</th>\n",
       "      <td>0.6944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bipolar Disorders</th>\n",
       "      <td>0.6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Mental Disorders</th>\n",
       "      <td>0.6642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Mean answer_relevancy for RAG\n",
       "psychiatric_category                                                             \n",
       "Dissociative Disorders                                                     0.8022\n",
       "Anxiety Disorders                                                          0.7822\n",
       "Eating Disorders                                                           0.7656\n",
       "Obsessive-Compulsive Disorders                                             0.7543\n",
       "Depressive Disorders                                                       0.7442\n",
       "Personality Disorders                                                      0.7439\n",
       "Trauma and Stressor Related Disorders                                      0.7069\n",
       "Schizophrenia Spectrum and Other Psychotic Diso...                         0.7028\n",
       "Somatic Disorders                                                          0.6944\n",
       "Bipolar Disorders                                                          0.6814\n",
       "Other Mental Disorders                                                     0.6642"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def process_files(file_name, model_name='Llama-4-Scout'):\n",
    "    '''\n",
    "    The function will output full report for the set of results for one model.\n",
    "\n",
    "    '''\n",
    "    base_name = file_name.replace('.json', '')\n",
    "\n",
    "    # Files with answer similarity results\n",
    "    VANILLA_ANSWER_SIMILARITY = f\"{base_name}_vanilla_answer_similarity_evaluated.json\"\n",
    "    RAG_ANSWER_SIMILARITY = f\"{base_name}_rag_answer_similarity_evaluated.json\"\n",
    "    RAG_FAITHFULNESS = f\"{base_name}_rag_faithfulness_evaluated.json\"\n",
    "    RAG_ANSWER_RELEVANCY = f\"{base_name}_rag_answer_relevancy_evaluated.json\"\n",
    "\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    print(f\"Evaluation of {model_name} Results:\\n\")\n",
    "\n",
    "    # ANSWER SIMILARITY - BOTH VANILA AND RAG\n",
    "  \n",
    "    \n",
    "    with open(VANILLA_ANSWER_SIMILARITY, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f) \n",
    "    vanilla_answer_similarity = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    with open(RAG_ANSWER_SIMILARITY, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f) \n",
    "    rag_answer_similarity = pd.DataFrame(data)\n",
    "    rag_answer_similarity = rag_answer_similarity[[\"Modified Questions\", \n",
    "            \"Answer Semantic Similarity for rag\"]]\n",
    "    \n",
    "    merged_df = pd.merge(vanilla_answer_similarity, rag_answer_similarity, on=\"Modified Questions\", how=\"inner\")\n",
    "    \n",
    "\n",
    "    # Calculate overall means\n",
    "    print(\"Answer Semantic Similarity Results:\")\n",
    "    vanilla_similarity_mean = merged_df['Answer Semantic Similarity for vanilla'].mean()\n",
    "    rag_similarity_mean = merged_df['Answer Semantic Similarity for rag'].mean()\n",
    "    print(f\"Overall Mean Answer Semantic Similarity for RAG: {rag_similarity_mean:.4f}\")\n",
    "    print(f\"Overall Mean Answer Semantic Similarity for Vanilla: {vanilla_similarity_mean:.4f}\")\n",
    "    print(f\"Difference (RAG - Vanilla): {rag_similarity_mean - vanilla_similarity_mean:.4f}\")\n",
    "\n",
    "    # Group by psychiatric category\n",
    "    category_stats_answer_similarity = merged_df.groupby('psychiatric_category').agg({\n",
    "            'Answer Semantic Similarity for vanilla': 'mean',\n",
    "            'Answer Semantic Similarity for rag': 'mean'}).round(4)\n",
    "        \n",
    "    \n",
    "    category_stats_answer_similarity.columns = ['Vanilla_Mean', 'RAG_Mean']  \n",
    "    # Add difference column\n",
    "    category_stats_answer_similarity['Difference (RAG - Vanilla)'] = (category_stats_answer_similarity['RAG_Mean'] - \n",
    "                                                    category_stats_answer_similarity['Vanilla_Mean']).round(4) \n",
    "    # Sort by difference\n",
    "    category_stats_answer_similarity = category_stats_answer_similarity.sort_values('Difference (RAG - Vanilla)', ascending=False)\n",
    "        \n",
    "    display(category_stats_answer_similarity)\n",
    "\n",
    "    \n",
    "    # EVALUATE RAG TRIAD - FAITFULNESS\n",
    "    with open(RAG_FAITHFULNESS, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f) \n",
    "    rag_faithfulness = pd.DataFrame(data)\n",
    "    rag_faithfulness = rag_faithfulness[[\"Modified Questions\", \n",
    "                \"faithfulness for RAG run 1\",\n",
    "                \"faithfulness for RAG run 2\",\n",
    "                \"faithfulness for RAG run 3\",\n",
    "                \"Mean faithfulness for RAG\"]]\n",
    "    \n",
    "    merged_df = pd.merge(merged_df, rag_faithfulness, on=\"Modified Questions\", how=\"inner\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    print(\"RAG Triad - Faithfulness Results:\")\n",
    "    faithfulness_mean = merged_df['Mean faithfulness for RAG'].mean()\n",
    "    print(f\"Overall Mean Faithfulness for RAG: {faithfulness_mean:.4f}\")\n",
    "\n",
    "    # Group by psychiatric category\n",
    "    category_stats_faithfulness = merged_df.groupby('psychiatric_category').agg({\n",
    "        'Mean faithfulness for RAG': 'mean',\n",
    "    }).round(4)\n",
    "    category_stats_faithfulness = category_stats_faithfulness.sort_values('Mean faithfulness for RAG', ascending=False)\n",
    "    display(category_stats_faithfulness)\n",
    "\n",
    "    # EVALUATE RAG TRIAD - ANSWER RELEVANCY\n",
    "\n",
    "    with open(RAG_ANSWER_RELEVANCY, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    rag_answer_relevancy = pd.DataFrame(data)\n",
    "    rag_answer_relevancy = rag_answer_relevancy[[\"Modified Questions\",\n",
    "                                                 \"answer_relevancy for RAG run 1\",\n",
    "                                                    \"answer_relevancy for RAG run 2\",\n",
    "                                                    \"answer_relevancy for RAG run 3\",\n",
    "                                                    \"Mean answer_relevancy for RAG\"]]\n",
    "    \n",
    "\n",
    "    merged_df = pd.merge(merged_df, rag_answer_relevancy, on=\"Modified Questions\", how=\"inner\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    print(\"RAG Triad - Answer Relevancy Results:\")\n",
    "    relevancy_mean = merged_df['Mean answer_relevancy for RAG'].mean()\n",
    "    print(f\"Overall Mean Relevancy for RAG: {relevancy_mean:.4f}\")\n",
    "\n",
    "    # Group by psychiatric category\n",
    "    category_stats_relevancy = merged_df.groupby('psychiatric_category').agg({\n",
    "        'Mean answer_relevancy for RAG': 'mean'}).round(4)\n",
    "    category_stats_relevancy = category_stats_relevancy.sort_values('Mean answer_relevancy for RAG', ascending=False)\n",
    "    display(category_stats_relevancy)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "process_files(\"test_dataset_together_meta-llama_Llama-4-Scout-17B-16E-Instruct_top5_answered.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
