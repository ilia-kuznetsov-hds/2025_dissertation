{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e712d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135038a7",
   "metadata": {},
   "source": [
    "Evaluation setup:  \n",
    "* Embedding model - Text Embedding 004 (Google)  \n",
    "* LLM - Gemini 2.5 Flash  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd1067",
   "metadata": {},
   "source": [
    "## RAGAS Answer Correctness:\n",
    "\n",
    "The assessment of Answer Correctness involves measuring the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness.\n",
    "Answer correctness  is computed as the sum of factual correctness and the semantic similarity between the given answer and the ground truth.  \n",
    "\n",
    "Factual correctness is a metric that compares and evaluates the factual accuracy of the generated response with the reference. This metric is used to determine the extent to which the generated response aligns with the reference. The factual correctness score ranges from 0 to 1, with higher values indicating better performance. To measure the alignment between the response and the reference, the metric uses the LLM to first break down the response and reference into claims and then uses natural language inference to determine the factual overlap between the response and the reference. Factual overlap is quantified using precision, recall, and F1 score, which can be controlled using the mode parameter. By default, the mode is set to F1, you can change the mode to precision or recall by setting the mode parameter.\n",
    "\n",
    "Answer similarity is calculated by following steps:  \n",
    "Step 1: Vectorize the ground truth answer using the embedding model.  \n",
    "Step 2: Vectorize the generated answer using the same embedding model.  \n",
    "Step 3: Compute the cosine similarity between the two vectors.  \n",
    "        \n",
    "By default \"text-embedding-ada-002\" model is used. In that evaluation, we used Text Embedding 004 (Google).  \n",
    "\n",
    "Final score is created by taking a weighted average of the factual correctness (F1 score) and the semantic similarity. \n",
    "(By default, there is a 0.75 : 0.25 weighting.)   \n",
    "\n",
    "Total API Calls: 4  \n",
    "* 1 LLM call to produce the \"simple statements  \n",
    "* 1 LLM call to determine the true positives, false positives, and false negatives  \n",
    "* 1 embedding call to embed the context  \n",
    "* 1 embedding call to embed the AI answer  \n",
    "\n",
    "### Unsuccesful experiment with Gemini 2.5 flash:  \n",
    "Because the generated results are non-deterministic, I run evaluation 3 times and calculate the mean. To evaluate 370 questions x 3 times x 2 (vanilla and RAG) X 4 API calls = 8880 API calls total for one set of results. When I decided to play with Gemini 2.5 flash, â€‹I also calculated other metrics, in total it cost 16000 API calls and 167 dollars.\n",
    "\n",
    "**Conclusion**: I will stick with Gemini 2.0 flash model as a main judge for the rest of the project.\n",
    "\n",
    "I feel like RAGAS use much more in terms of LLM calls than just 4 as they say in documentation. I didn't examine it properly, but there is a [source code](https://github.com/explodinggradients/ragas/blob/main/ragas/src/ragas/metrics/_answer_correctness.py).\n",
    "There is a [compaint](https://www.reddit.com/r/LangChain/comments/1dbmqii/i_spent_700_on_evaluating_100_rag_qa_set_using/) about RAGAS costing 700$ for 100 QA set.      \n",
    "\n",
    "\n",
    "Sources:  \n",
    "* [RAGAS Docs for answer correctness](https://docs.ragas.io/en/v0.1.21/concepts/metrics/answer_correctness.html)  \n",
    " \n",
    "https://github.com/dkhundley/llm-rag-guide/blob/main/notebooks/ragas.ipynb  \n",
    "https://docs.ragas.io/en/stable/references/embeddings/#ragas.embeddings.embedding_factory  \n",
    "https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/factual_correctness/#factual-correctness  \n",
    "https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/semantic_similarity/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48695377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean Answer Correctness for Vanilla: 0.6058\n",
      "Overall Mean Answer Correctness for RAG: 0.5695\n",
      "Difference (RAG - Vanilla): -0.0364\n"
     ]
    }
   ],
   "source": [
    "file_name = \"test_dataset_together_meta-llama_Llama-4-Scout-17B-16E-Instruct_top5_answered.json\"\n",
    "base_name = file_name.replace('.json', '')\n",
    "\n",
    "    # Use forward slashes or os.path.join for better cross-platform compatibility\n",
    "VANILLA_ANSWER_CORRECTNESS = f\"{base_name}_vanilla_answer_correctness_evaluated.json\"\n",
    "RAG_ANSWER_CORRECTNESS = f\"{base_name}_rag_answer_correctness_evaluated.json\"\n",
    "RAG_ANSWER_SIMILARITY = f\"{base_name}_rag_answer_similarity_evaluated.json\"\n",
    "VANILLA_ANSWER_SIMILARITY = f\"{base_name}_vanilla_answer_similarity_evaluated.json\"\n",
    "RAG_ANSWER_RELEVANCY = f\"{base_name}_rag_answer_relevancy_evaluated.json\"\n",
    "RAG_FAITHFULNESS = f\"{base_name}_rag_faithfulness_evaluated.json\"\n",
    "\n",
    "with open(VANILLA_ANSWER_CORRECTNESS, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)   \n",
    "df_1 = pd.DataFrame(data)\n",
    "\n",
    "with open(RAG_ANSWER_CORRECTNESS, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f) \n",
    "df_2 = pd.DataFrame(data)\n",
    "df_2 = df_2[[\"Modified Questions\", \n",
    "                 \"Answer Correctness for RAG run 1\", \n",
    "                 \"Answer Correctness for RAG run 2\",\n",
    "                 \"Answer Correctness for RAG run 3\",\n",
    "                 \"Mean Answer Correctness for RAG\"]]\n",
    "    \n",
    "merged_df = pd.merge(df_1, df_2, on=\"Modified Questions\", how=\"inner\")\n",
    "# Calculate overall means\n",
    "vanilla_mean = merged_df['Mean Answer Correctness for vanilla'].mean()\n",
    "rag_mean = merged_df['Mean Answer Correctness for RAG'].mean()\n",
    "    \n",
    "print(f\"Overall Mean Answer Correctness for Vanilla: {vanilla_mean:.4f}\")\n",
    "print(f\"Overall Mean Answer Correctness for RAG: {rag_mean:.4f}\")\n",
    "print(f\"Difference (RAG - Vanilla): {rag_mean - vanilla_mean:.4f}\")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f77719a",
   "metadata": {},
   "source": [
    "Again, vanilla LLM response is evaluated higher than RAG-enhanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b0eef0",
   "metadata": {},
   "source": [
    "## RAGAS Answer semantic similarity  \n",
    "\n",
    "This evaluation is based on the ground truth and the answer, with values falling within the range of 0 to 1. A higher score signifies a better alignment \n",
    "between the generated answer and the ground truth.  \n",
    "Step 1: Vectorize the ground truth answer using the specified embedding model.  \n",
    "Step 2: Vectorize the generated answer using the same embedding model.  \n",
    "Step 3: Compute the cosine similarity between the two vectors.  \n",
    "\n",
    "The metric is a part of RAGAS Answer Correctness metric. The Answer Correctness's final score is created by taking a weighted average of the factual correctness (F1 score) and the semantic similarity. \n",
    "(By default, there is a 0.75 : 0.25 weighting.)   \n",
    "\n",
    "\n",
    "Total API Calls: 2\n",
    "* 1 embedding call to embed the ground truth\n",
    "* 1 embedding call to embed the AI answer  \n",
    "\n",
    "The metric is produced much faster (around 30 minutes for the whole set of results). It is 25% component of Answer Correctness metric.  \n",
    "Since the output is deterministic, we run evaluation only once for each question.   \n",
    "\n",
    "Sources:  \n",
    "* [Ragas Docs for semantic similarity](https://docs.ragas.io/en/v0.1.21/concepts/metrics/semantic_similarity.html)   \n",
    "* https://github.com/dkhundley/llm-rag-guide/blob/main/notebooks/ragas.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32659d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean Answer Semantic Similarity for RAG: 0.8752\n",
      "Overall Mean Answer Semantic Similarity for Vanilla: 0.8794\n",
      "Difference (RAG - Vanilla): -0.0042\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vanilla_Mean</th>\n",
       "      <th>RAG_Mean</th>\n",
       "      <th>Difference (RAG - Vanilla)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychiatric_category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Eating Disorders</th>\n",
       "      <td>0.8511</td>\n",
       "      <td>0.8656</td>\n",
       "      <td>0.0145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Somatic Disorders</th>\n",
       "      <td>0.8635</td>\n",
       "      <td>0.8762</td>\n",
       "      <td>0.0127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality Disorders</th>\n",
       "      <td>0.8951</td>\n",
       "      <td>0.9062</td>\n",
       "      <td>0.0111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anxiety Disorders</th>\n",
       "      <td>0.8841</td>\n",
       "      <td>0.8850</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Mental Disorders</th>\n",
       "      <td>0.8745</td>\n",
       "      <td>0.8709</td>\n",
       "      <td>-0.0036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dissociative Disorders</th>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.9323</td>\n",
       "      <td>-0.0041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bipolar Disorders</th>\n",
       "      <td>0.8830</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>-0.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trauma and Stressor Related Disorders</th>\n",
       "      <td>0.8932</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>-0.0083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Schizophrenia Spectrum and Other Psychotic Disorders</th>\n",
       "      <td>0.8835</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>-0.0083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depressive Disorders</th>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.8630</td>\n",
       "      <td>-0.0105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obsessive-Compulsive Disorders</th>\n",
       "      <td>0.9039</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>-0.0228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Vanilla_Mean  RAG_Mean  \\\n",
       "psychiatric_category                                                         \n",
       "Eating Disorders                                          0.8511    0.8656   \n",
       "Somatic Disorders                                         0.8635    0.8762   \n",
       "Personality Disorders                                     0.8951    0.9062   \n",
       "Anxiety Disorders                                         0.8841    0.8850   \n",
       "Other Mental Disorders                                    0.8745    0.8709   \n",
       "Dissociative Disorders                                    0.9364    0.9323   \n",
       "Bipolar Disorders                                         0.8830    0.8783   \n",
       "Trauma and Stressor Related Disorders                     0.8932    0.8849   \n",
       "Schizophrenia Spectrum and Other Psychotic Diso...        0.8835    0.8752   \n",
       "Depressive Disorders                                      0.8735    0.8630   \n",
       "Obsessive-Compulsive Disorders                            0.9039    0.8811   \n",
       "\n",
       "                                                    Difference (RAG - Vanilla)  \n",
       "psychiatric_category                                                            \n",
       "Eating Disorders                                                        0.0145  \n",
       "Somatic Disorders                                                       0.0127  \n",
       "Personality Disorders                                                   0.0111  \n",
       "Anxiety Disorders                                                       0.0009  \n",
       "Other Mental Disorders                                                 -0.0036  \n",
       "Dissociative Disorders                                                 -0.0041  \n",
       "Bipolar Disorders                                                      -0.0047  \n",
       "Trauma and Stressor Related Disorders                                  -0.0083  \n",
       "Schizophrenia Spectrum and Other Psychotic Diso...                     -0.0083  \n",
       "Depressive Disorders                                                   -0.0105  \n",
       "Obsessive-Compulsive Disorders                                         -0.0228  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with open(RAG_ANSWER_SIMILARITY, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f) \n",
    "df_3 = pd.DataFrame(data)\n",
    "df_3 = df_3[[\"Modified Questions\", \n",
    "            \"Answer Semantic Similarity for rag\"]]\n",
    "\n",
    "\n",
    "with open(VANILLA_ANSWER_SIMILARITY, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f) \n",
    "df_4 = pd.DataFrame(data)\n",
    "df_4 = df_4[[\"Modified Questions\", \n",
    "            \"Answer Semantic Similarity for vanilla\"]]\n",
    "\n",
    "merged_df = pd.merge(merged_df, df_3, on=\"Modified Questions\", how=\"inner\")\n",
    "merged_df = pd.merge(merged_df, df_4, on=\"Modified Questions\", how=\"inner\")\n",
    "\n",
    "# Calculate overall means\n",
    "rag_similarity_mean = merged_df['Answer Semantic Similarity for rag'].mean()\n",
    "vanilla_similarity_mean = merged_df['Answer Semantic Similarity for vanilla'].mean()\n",
    "print(f\"Overall Mean Answer Semantic Similarity for RAG: {rag_similarity_mean:.4f}\")\n",
    "print(f\"Overall Mean Answer Semantic Similarity for Vanilla: {vanilla_similarity_mean:.4f}\")\n",
    "print(f\"Difference (RAG - Vanilla): {rag_similarity_mean - vanilla_similarity_mean:.4f}\")\n",
    "\n",
    "# Group by psychiatric category\n",
    "category_stats = merged_df.groupby('psychiatric_category').agg({\n",
    "        'Answer Semantic Similarity for vanilla': 'mean',\n",
    "        'Answer Semantic Similarity for rag': 'mean'}).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "category_stats.columns = ['Vanilla_Mean', 'RAG_Mean']\n",
    "    \n",
    "    # Add difference column\n",
    "category_stats['Difference (RAG - Vanilla)'] = (category_stats['RAG_Mean'] - \n",
    "                                                   category_stats['Vanilla_Mean']).round(4)\n",
    "    \n",
    "    # Sort by difference to see which categories benefit most from RAG\n",
    "category_stats = category_stats.sort_values('Difference (RAG - Vanilla)', ascending=False)\n",
    "    \n",
    "display(category_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1612bd7",
   "metadata": {},
   "source": [
    "# RAG Metrics  \n",
    "## RAGAS Faithfulness \n",
    "Faithfulness measures the factual consistency of the generated answer against the given context. It is calculated from the answer and the retrieved context. The answer is scaled to the (0, 1) range. The higher the better. \n",
    "The generated answer is regarded as faithful if all the claims made in the answer can be inferred from the given context.  \n",
    "At the first step, the generated answer is broken down into individual statements.     \n",
    "Then each of these claims is cross-checked with the given context to determine if it can be inferred from the context.    \n",
    "The final score is calculated by dividing the number of claims that can be inferred from the context by the total number of claims in the generated response.  \n",
    "\n",
    "\n",
    "\n",
    "Groundedness, sometimes referred to as faithfulness, measures whether the response is based completely on the context. It validates that the response isn't using information other than what exists in the context. A low groundedness metric indicates that the language model might be outputting inaccurate or nonsensical responses.  A low groundedness calculation indicates that the language model doesn't see the chunks as relevant. You should evaluate whether you need to add data to your collection, adjust your chunking strategy or chunk size, or fine-tune your prompt.  \n",
    "\n",
    "Source:  \n",
    "* [RAGAS Docs for Faitfulness](https://docs.ragas.io/en/v0.1.21/concepts/metrics/faithfulness.html)   \n",
    "* https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-llm-evaluation-phase  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a435f6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean Faithfulness for RAG: 0.2678\n"
     ]
    }
   ],
   "source": [
    "with open(RAG_FAITHFULNESS, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f) \n",
    "df_3 = pd.DataFrame(data)\n",
    "df_3 = df_3[[\"Modified Questions\", \n",
    "                \"faithfulness for RAG run 1\",\n",
    "                \"faithfulness for RAG run 2\",\n",
    "                \"faithfulness for RAG run 3\",\n",
    "                \"Mean faithfulness for RAG\"]]\n",
    "    \n",
    "merged_df = pd.merge(merged_df, df_3, on=\"Modified Questions\", how=\"inner\")\n",
    "\n",
    "faithfulness_mean = merged_df['Mean faithfulness for RAG'].mean()\n",
    "print(f\"Overall Mean Faithfulness for RAG: {faithfulness_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f09b26",
   "metadata": {},
   "source": [
    "Only 26% of retrieved statements are used. "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ4AAABJCAIAAABD+V+SAAANcklEQVR4Ae2cf1BUVRvHjxMLg7FOMOxOhS6SSVQz7ziMSzNSTMFsM2EgkExGphVY01sjIhoT0zhqYwjuTjoYOC0hBkFLlkOzIYSBlog7EoQoJr/awmJdQH4swu5e2fOWdzrv7bK73l3uxXV9zh96fjznOed+zv3uPefec0AYAhAAAhwIIA42YAIEgAAGqcBNAAQ4EQCpcMIERkAApAL3ABDgRACkwgkTGAEBkArcA0CAEwGQCidMYAQEQCpwDwABTgRAKpwwgREQAKnAPQAEOBEAqXDCBEZAAKQC9wAQ4EQApMIJExgBAZAK3ANAgBMBkAonTGAEBEAqHnoP9Pb2ajQai8XS1dVVWVnZ09PjoR29a7oFUvHQoT548KBKpXrxxRebm5tv3LgRGRk5MzPjoX29O7oFUvHEcZ6amrp48WJ6erparcYYm81mqVTqiR29m/oEUhF2tP/44w+TyeReG8uXL7969SrGuL6+fs2aNe45gVp8EQCp8EXyX35MJlNjY2NBQUFgYODp06f/VcYt8eeff0ZERNC2r7322hdffFFZWXnjxg1utcGKfwIgFf6ZYoxHRkYaGxvHxsYQQj/++KMbbWi12szMTLriu+++q1QqNRqNG36gCl8EQCp8kbTjh6Iot6Vis9mYz5DJyUk7DUDWPBIAqQgIey5SEbBb4NotAiAVt7BxqwRS4cbpzrACqQg4TiAVAeHOu2uQioDIQSoCwp131yAVAZFzkcozzzyDboawsLD/OA6PPvrokiVL/P39aWP6X6lUarFYBLwAcM0gAFJhwOA7ykUqOp3Ox8cHIbR69Wou7Q8ODtbU1KSnp/v5+SGEysvLudQCm7kTAKnMnaFDD1NTUwihpqYmhxY3C/Lz8+mnxP79+51bMkuNRmNGRsbKlSuZmRAXjgBIRRC2IyMjubm5L7300sKFC+Pi4t5777329nZHLdlsNoVCgRASiURtbW2OzOzmZ2VltbS02C3iMdNqtVZXV/PokOXq2LFjU1NTrExPS3qhVOzeOjqdruWfoNPpBgYGbDab5wyGwWCQSqUIoeXLl7u0Z8xisZSVld3yQoxGY15entu341tvvdXX13fLVtw2GB4efuONNzx867S3SWVsbEwsFvf397OG7fz58ytXrkQIrV+/XqVSbdq0SSKRrFu3zmg0sixvV7K+vp6ehm3YsIH3Pmi12gULFly4cMENz2VlZfOwImpoaFAqlW50b96qeJtUPvroI4QQ2T3F5JiTk/PX6vncuXN0ZmtrK0IoISGBaXN749u2baPVUlFRwXtPxsfH3fBpsViioqKYW2zccMKxytNPPz02NsbReP7NvEoqNpstNjY2MTFRLBZPTEywaObm5jKlgjFevHixv78/y+w2Jq1Wq1wuRwgFBAR4yLHH2tranTt3zg+TAwcOCPEbwVfnvUoqtbW1eXl5TU1NCKHZb5NYUpmamhKJRMuWLeMLJS9++vr6xGIxQigyMtLtbyZfffXV3r17i4uLs7OzKYrCGP/888/Z2dk6nQ5jPDg4WFRUtHnzZqPRmJWVpVAojhw5gjHWaDRJN4NeryfXkpGRceLECZIkkXPnzh05cqSvr298fPyWqrZYLN999115efnExMTly5cdLcba2tqSk5NJE54W8SqpJCUl0WuPxx9/PCwsjLVMZEqlr68vISHBz8/vm2++sTskZrP57Nmz/7wIsPN/Z2en3Ypzz6ysrKSnYVu3bnXDW0dHB3mDvGHDhomJCb1en52djRA6fvw4xri7u/vJJ58MDg7Oz8/v6urauXPnPffcs2XLluPHj//000+RkZHMY2RLly4dHR1ldoOiqNdff720tBRjrFarY2JiyJyWaUbiBoMhNja2o6PDarXm5eWFh4fT0zmKolhP/pmZmfvuu49U9LSI90ilt7eXLIiLi4sRQseOHWPipqUSHx8fERGBENq1axfz55NpiTE2GAwqlWqf41BSUsKqwmNy48aNtFpqa2tddavVaiUSCX1pHR0d9BLl8uXLRCoY46ysLJlMRnseGhpCCB04cIBOfvDBB/fffz9pNCAggMTpyLZt29LS0uh4a2urWCx2spKx2WxRUVH0sWeMsVKpfO655+i69fX1e/fuZTkPCgry2OMG3iOVLVu2ZGdnl98Mhw4dQgjFxMQwR4I8VbRaLUIoMTFRiPfFJSUlcY5Dbm4us0uO4pOTk+Hh4Qih4ODga9euOTKzm282m0NDQwMCAlQqFT37whj39PQwpbJ161YiFYwxQqiwsJD2tm/fPj8/PzpuMpmCgoKYrfT39/v4+JBvREVFReTWZ5qReEVFRVBQENFSampqQUEBKZ0defjhh3/99dfZ+Z6Q4yVSuX79+qpVq6oYYfXq1QghMqgYYyIVEn///fc9YQzs9qG9vd3X1zc2Npbc7nbN7GYODg4mJSUhhJ566qnh4WFXpeLr60u7pShKLBYzm1Cr1RKJhOSkpqbm5+eT5OxIWlra2rVrSb5UKqXXS2fOnKmrqyP5JCKTyQwGA0l6VMRLpFJYWHjw4EEm2fb2doQQmZIRedAT65mZmbi4OISQo4/Qer0+OTk50XF4++23mc3xHq+pqXnsscdY6wQurbS2ttKLtLq6ukWLFu3YscNtqWCMmc8EjPH27dvJY8RmswUHB589e9ZJr+RyOXmMXLhw4d5776Uoqrm5WafTxcfH//bbb6y6gYGB5BHEKrrtSW+QitlsXrp06cjICIvmI488IhKJfv/9dzo/KysLIUT+KITRaAwJCfH392c+eVgebleyra0tNDTUvQ/khw8fPnnyJN3zjRs3bt++HWN86dIlhJBWq6Xz33nnHTIBs1gszBeG+fn5IpGIXHh0dHRXVxdJfvrpp0lJSXSyoqJi4cKF5KF3+vTpdevWsbS9fv16sgpKT09/9tlnMcatra2Tk5OhoaGkLu3QaDSSv7xBWvScyB0vlatXr6ampiKEdu/ezdy4UV1dLZPJEEJRUVFtbW1qtfrBBx9ECCkUivr6enoAWlpafHx8AgMDP/74Y88ZkitXroSFhTU3N7vXpbKyMrlc/uWXX1ZVVcXHxw8NDfX29r755psIoZSUlK6uru+//37ZsmULFiwoKSkxmUx79uxBCMnl8jNnzpw/fz46Oppe5dMwlUrlZ599RnpiNpsTEhK0Wm1paWlKSopCoSBFGo3mgQceYL1K6e7uTkpKOnHiRGFh4YoVKz788EPavrS0dPPmzaQuHdFqtTk5OaxMz0ne8VLxHJS89GRycnLFihWVlZVue6N/qkdHR8nj1G1XGOP+/n7mu2PalV6vpyhq7dq1e/bsYTofGxurqqpi5vw177VarXq93mazSSQSov+YmJj29nbWfpmMjAx6JcPy4CFJkIqHDMTf3ZiZmXn++efn7es4xyt/4YUXrly5MttYIpGQ2SxdWltbO3v3HV108eJFf39/q9VKJ5OTkz///POGhgbi9tq1a2QVRDI9KgJS8aDhyMzMJJ8suHfL0ZsJ7h6cW+r1+ldffZVp093drVQq/5q2ffvtt2RLwfT0tEqlYpqReEtLyyuvvPLQQw8xP1Zev36dGGCMMzMz3dvNyXQiaBykIiheF5wXFRVFR0ebzWYX6mB86NAh1vLApeocjRsbG5mfXDs7O5uamk6dOtXQ0EB2qZhMJiIbltsffvjh1KlTJ0+edHTK7ejRozU1NaxanpYEqfA/Ilar9ejRo/Rb1IGBgcOHD3/99dfT09NOWqqrqwsPDx8aGnJiwyoaHx/ftWuXVColsxqWAb9JV7+EutS6oM5d6okTY5CKEzhuFhUXF4+Ojsrl8qqqKo1GQ1GURqNxcnS+s7NTJpP98ssvXNqjKKqzszMnJ2fRokWOjhtw8QM2rhIAqbhK7Nb29NamkJAQ8oKopaUlLCzMbk2DwSCTyZYsWRLrNKxatSoyMnLx4sX036ygd4ghhDo6Ouy6hUzeCYBUeEf691bL3t5ekUhEVq6FhYUpKSl2W9q/f79TjTgrZO5FsOscMnkkAFLhEeb/XanV6ujoaJJWKBRkPyLJhMidRQCkIsh4paWlkU3EBoPB19fXYDD09PTcEetXQYjc+U5BKoKMYUhICPm+Vl5e/sQTT2CMd+/eLcS2f0EuAJzOIgBSmYVkzhnDw8NisZhsSLt06dKaNWuqq6udb8KdfSrQeUfsfkF3XgVK50IApDIXeg7rsiZaExMTRDmO6tg9FTjb2GKxFBQUbNq06eWXX55dCjnCEQCpCMdWQM9VVVVubIERsEN3gWuQikcMsqNTgY46B1JxREa4fJCKcGy5emadCiwtLf2vvcA83gxS4QqXPzuQCn8s3fXk6FSgE38gFSdwBCoCqQgE1jW3zFOB09PTo/YC2cOLMQapuMaXD2uQCh8U5+yDeSpQo9HssBeYp0FAKnNG7rIDkIrLyISoMPtUoJNWPvnkk+Tk5IiIiOLi4oGBASeWUMQjAZAKjzDn5IrsrZyTF6gsGAGQimBowbF3EQCpeNd4wtUIRgCkIhhacOxdBEAq3jWecDWCEQCpCIYWHHsXAZCKd40nXI1gBEAqgqEFx95FAKTiXeMJVyMYAZCKYGjBsXcR+B+7W/RMYISUZwAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "be9cff34",
   "metadata": {},
   "source": [
    "## RAGAS Answer Relevance  \n",
    "\n",
    "The Answer Relevance metric evaluates to what extent the generated answer address the provided question. The answer is considered relevant if it directly addressess the question.    \n",
    "Step 1: Reverse-engineer â€˜nâ€™ variants of the question from the generated answer using a LLM (prompt: \"Generate a question for the given answer.\n",
    "answer: [answer])\")  \n",
    "Step 2: Generate embedding for all the questions. Calculate the mean cosine similarity between the generated questions and the actual question by the following formula:     \n",
    "![image.png](attachment:image.png)  \n",
    "\n",
    "The Answer Relevance doesn't assess factual correctness of generated answer but rather penalises redundant or insufficient answers.  \n",
    "\n",
    "Total API Calls by default: 4  \n",
    "* 1 LLM call to generate the question based on the answer (by default - 3 question)  \n",
    "* 1 embedding call for each generated question   (by default - 3) \n",
    "* 1 embedding call to embed the original question  \n",
    "\n",
    "Sources:  \n",
    "* [RAGAS Documentation](https://docs.ragas.io/en/v0.1.21/concepts/metrics/answer_relevance.html)  \n",
    "* [Original RAGAS paper](https://arxiv.org/abs/2309.15217)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3916febd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean Relevancy for RAG: 0.7071\n",
      "                                                    RAG_Mean\n",
      "psychiatric_category                                        \n",
      "Dissociative Disorders                                0.8022\n",
      "Anxiety Disorders                                     0.7822\n",
      "Eating Disorders                                      0.7656\n",
      "Obsessive-Compulsive Disorders                        0.7543\n",
      "Depressive Disorders                                  0.7442\n",
      "Personality Disorders                                 0.7439\n",
      "Trauma and Stressor Related Disorders                 0.7069\n",
      "Schizophrenia Spectrum and Other Psychotic Diso...    0.7028\n",
      "Somatic Disorders                                     0.6944\n",
      "Bipolar Disorders                                     0.6814\n",
      "Other Mental Disorders                                0.6642\n"
     ]
    }
   ],
   "source": [
    "with open(RAG_ANSWER_RELEVANCY, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f) \n",
    "\n",
    "df_4 = pd.DataFrame(data)\n",
    "df_4 = df_4[[\"Modified Questions\",\n",
    "            \"answer_relevancy for RAG run 1\",\n",
    "    \"answer_relevancy for RAG run 2\",\n",
    "    \"answer_relevancy for RAG run 3\",\n",
    "    \"Mean answer_relevancy for RAG\"]]\n",
    "merged_df = pd.merge(merged_df, df_4, on=\"Modified Questions\", how=\"inner\")\n",
    "\n",
    "relevancy_mean = merged_df['Mean answer_relevancy for RAG'].mean()\n",
    "print(f\"Overall Mean Relevancy for RAG: {relevancy_mean:.4f}\")\n",
    "\n",
    "# Group by psychiatric category\n",
    "category_stats = merged_df.groupby('psychiatric_category').agg({\n",
    "        'Mean answer_relevancy for RAG': 'mean'}).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "category_stats.columns = ['RAG_Mean']\n",
    "       \n",
    "# Sort by difference to see which categories benefit most from RAG\n",
    "category_stats = category_stats.sort_values('RAG_Mean', ascending=False)\n",
    "    \n",
    "print(category_stats)\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcab57c",
   "metadata": {},
   "source": [
    "## Final function to generate report \n",
    "(work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e703c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Evaluation of Llama-4-Scout Results:\n",
      "\n",
      "Answer Semantic Similarity Results:\n",
      "Overall Mean Answer Semantic Similarity for RAG: 0.8752\n",
      "Overall Mean Answer Semantic Similarity for Vanilla: 0.8794\n",
      "Difference (RAG - Vanilla): -0.0042\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vanilla_Mean</th>\n",
       "      <th>RAG_Mean</th>\n",
       "      <th>Difference (RAG - Vanilla)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychiatric_category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Eating Disorders</th>\n",
       "      <td>0.8511</td>\n",
       "      <td>0.8656</td>\n",
       "      <td>0.0145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Somatic Disorders</th>\n",
       "      <td>0.8635</td>\n",
       "      <td>0.8762</td>\n",
       "      <td>0.0127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality Disorders</th>\n",
       "      <td>0.8951</td>\n",
       "      <td>0.9062</td>\n",
       "      <td>0.0111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anxiety Disorders</th>\n",
       "      <td>0.8841</td>\n",
       "      <td>0.8850</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Mental Disorders</th>\n",
       "      <td>0.8745</td>\n",
       "      <td>0.8709</td>\n",
       "      <td>-0.0036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dissociative Disorders</th>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.9323</td>\n",
       "      <td>-0.0041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bipolar Disorders</th>\n",
       "      <td>0.8830</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>-0.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trauma and Stressor Related Disorders</th>\n",
       "      <td>0.8932</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>-0.0083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Schizophrenia Spectrum and Other Psychotic Disorders</th>\n",
       "      <td>0.8835</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>-0.0083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depressive Disorders</th>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.8630</td>\n",
       "      <td>-0.0105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obsessive-Compulsive Disorders</th>\n",
       "      <td>0.9039</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>-0.0228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Vanilla_Mean  RAG_Mean  \\\n",
       "psychiatric_category                                                         \n",
       "Eating Disorders                                          0.8511    0.8656   \n",
       "Somatic Disorders                                         0.8635    0.8762   \n",
       "Personality Disorders                                     0.8951    0.9062   \n",
       "Anxiety Disorders                                         0.8841    0.8850   \n",
       "Other Mental Disorders                                    0.8745    0.8709   \n",
       "Dissociative Disorders                                    0.9364    0.9323   \n",
       "Bipolar Disorders                                         0.8830    0.8783   \n",
       "Trauma and Stressor Related Disorders                     0.8932    0.8849   \n",
       "Schizophrenia Spectrum and Other Psychotic Diso...        0.8835    0.8752   \n",
       "Depressive Disorders                                      0.8735    0.8630   \n",
       "Obsessive-Compulsive Disorders                            0.9039    0.8811   \n",
       "\n",
       "                                                    Difference (RAG - Vanilla)  \n",
       "psychiatric_category                                                            \n",
       "Eating Disorders                                                        0.0145  \n",
       "Somatic Disorders                                                       0.0127  \n",
       "Personality Disorders                                                   0.0111  \n",
       "Anxiety Disorders                                                       0.0009  \n",
       "Other Mental Disorders                                                 -0.0036  \n",
       "Dissociative Disorders                                                 -0.0041  \n",
       "Bipolar Disorders                                                      -0.0047  \n",
       "Trauma and Stressor Related Disorders                                  -0.0083  \n",
       "Schizophrenia Spectrum and Other Psychotic Diso...                     -0.0083  \n",
       "Depressive Disorders                                                   -0.0105  \n",
       "Obsessive-Compulsive Disorders                                         -0.0228  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "RAG Triad - Faithfulness Results:\n",
      "Overall Mean Faithfulness for RAG: 0.2678\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean faithfulness for RAG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychiatric_category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bipolar Disorders</th>\n",
       "      <td>0.3454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depressive Disorders</th>\n",
       "      <td>0.3222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obsessive-Compulsive Disorders</th>\n",
       "      <td>0.3119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anxiety Disorders</th>\n",
       "      <td>0.3085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Schizophrenia Spectrum and Other Psychotic Disorders</th>\n",
       "      <td>0.2932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eating Disorders</th>\n",
       "      <td>0.2464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Mental Disorders</th>\n",
       "      <td>0.2375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality Disorders</th>\n",
       "      <td>0.2359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trauma and Stressor Related Disorders</th>\n",
       "      <td>0.1550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Somatic Disorders</th>\n",
       "      <td>0.1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dissociative Disorders</th>\n",
       "      <td>0.1190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Mean faithfulness for RAG\n",
       "psychiatric_category                                                         \n",
       "Bipolar Disorders                                                      0.3454\n",
       "Depressive Disorders                                                   0.3222\n",
       "Obsessive-Compulsive Disorders                                         0.3119\n",
       "Anxiety Disorders                                                      0.3085\n",
       "Schizophrenia Spectrum and Other Psychotic Diso...                     0.2932\n",
       "Eating Disorders                                                       0.2464\n",
       "Other Mental Disorders                                                 0.2375\n",
       "Personality Disorders                                                  0.2359\n",
       "Trauma and Stressor Related Disorders                                  0.1550\n",
       "Somatic Disorders                                                      0.1253\n",
       "Dissociative Disorders                                                 0.1190"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_dataset_together_meta-llama_Llama-4-Scout-17B-16E-Instruct_top5_answered_vanilla_answer_relevancy_evaluated.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 130\u001b[0m\n\u001b[0;32m    124\u001b[0m     category_stats_relevancy \u001b[38;5;241m=\u001b[39m category_stats_relevancy\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean answer_relevancy for RAG\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    125\u001b[0m     display(category_stats_relevancy)\n\u001b[1;32m--> 130\u001b[0m \u001b[43mprocess_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_dataset_together_meta-llama_Llama-4-Scout-17B-16E-Instruct_top5_answered.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 86\u001b[0m, in \u001b[0;36mprocess_files\u001b[1;34m(file_name, model_name)\u001b[0m\n\u001b[0;32m     82\u001b[0m display(category_stats_faithfulness)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# EVALUATE RAG TRIAD - ANSWER RELEVANCY\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mVANILLA_ANSWER_RELEVANCE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     87\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     88\u001b[0m vanilla_answer_relevancy \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "File \u001b[1;32mc:\\Users\\kuzne\\Documents\\Python_repo\\2025_01_dissertation\\2025_dissertation\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_dataset_together_meta-llama_Llama-4-Scout-17B-16E-Instruct_top5_answered_vanilla_answer_relevancy_evaluated.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_files(file_name, model_name='Llama-4-Scout'):\n",
    "    '''\n",
    "    The function will output full report for the set of results for one model.\n",
    "\n",
    "    '''\n",
    "    base_name = file_name.replace('.json', '')\n",
    "\n",
    "    # Files with answer similarity results\n",
    "    VANILLA_ANSWER_SIMILARITY = f\"{base_name}_vanilla_answer_similarity_evaluated.json\"\n",
    "    RAG_ANSWER_SIMILARITY = f\"{base_name}_rag_answer_similarity_evaluated.json\"\n",
    "    RAG_FAITHFULNESS = f\"{base_name}_rag_faithfulness_evaluated.json\"\n",
    "    VANILLA_ANSWER_RELEVANCE = f\"{base_name}_vanilla_answer_relevancy_evaluated.json\"\n",
    "    RAG_ANSWER_RELEVANCE = f\"{base_name}_rag_answer_relevancy_evaluated.json\"\n",
    "\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    print(f\"Evaluation of {model_name} Results:\\n\")\n",
    "\n",
    "    # ANSWER SIMILARITY - BOTH VANILA AND RAG\n",
    "  \n",
    "    \n",
    "    with open(VANILLA_ANSWER_SIMILARITY, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f) \n",
    "    vanilla_answer_similarity = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    with open(RAG_ANSWER_SIMILARITY, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f) \n",
    "    rag_answer_similarity = pd.DataFrame(data)\n",
    "    rag_answer_similarity = rag_answer_similarity[[\"Modified Questions\", \n",
    "            \"Answer Semantic Similarity for rag\"]]\n",
    "    \n",
    "    merged_df = pd.merge(vanilla_answer_similarity, rag_answer_similarity, on=\"Modified Questions\", how=\"inner\")\n",
    "    \n",
    "\n",
    "    # Calculate overall means\n",
    "    print(\"Answer Semantic Similarity Results:\")\n",
    "    vanilla_similarity_mean = merged_df['Answer Semantic Similarity for vanilla'].mean()\n",
    "    rag_similarity_mean = merged_df['Answer Semantic Similarity for rag'].mean()\n",
    "    print(f\"Overall Mean Answer Semantic Similarity for RAG: {rag_similarity_mean:.4f}\")\n",
    "    print(f\"Overall Mean Answer Semantic Similarity for Vanilla: {vanilla_similarity_mean:.4f}\")\n",
    "    print(f\"Difference (RAG - Vanilla): {rag_similarity_mean - vanilla_similarity_mean:.4f}\")\n",
    "\n",
    "    # Group by psychiatric category\n",
    "    category_stats_answer_similarity = merged_df.groupby('psychiatric_category').agg({\n",
    "            'Answer Semantic Similarity for vanilla': 'mean',\n",
    "            'Answer Semantic Similarity for rag': 'mean'}).round(4)\n",
    "        \n",
    "    \n",
    "    category_stats_answer_similarity.columns = ['Vanilla_Mean', 'RAG_Mean']  \n",
    "    # Add difference column\n",
    "    category_stats_answer_similarity['Difference (RAG - Vanilla)'] = (category_stats_answer_similarity['RAG_Mean'] - \n",
    "                                                    category_stats_answer_similarity['Vanilla_Mean']).round(4) \n",
    "    # Sort by difference\n",
    "    category_stats_answer_similarity = category_stats_answer_similarity.sort_values('Difference (RAG - Vanilla)', ascending=False)\n",
    "        \n",
    "    display(category_stats_answer_similarity)\n",
    "\n",
    "    \n",
    "    # EVALUATE RAG TRIAD - FAITFULNESS\n",
    "    with open(RAG_FAITHFULNESS, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f) \n",
    "    rag_faithfulness = pd.DataFrame(data)\n",
    "    rag_faithfulness = rag_faithfulness[[\"Modified Questions\", \n",
    "                \"faithfulness for RAG run 1\",\n",
    "                \"faithfulness for RAG run 2\",\n",
    "                \"faithfulness for RAG run 3\",\n",
    "                \"Mean faithfulness for RAG\"]]\n",
    "    \n",
    "    merged_df = pd.merge(merged_df, rag_faithfulness, on=\"Modified Questions\", how=\"inner\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    print(\"RAG Triad - Faithfulness Results:\")\n",
    "    faithfulness_mean = merged_df['Mean faithfulness for RAG'].mean()\n",
    "    print(f\"Overall Mean Faithfulness for RAG: {faithfulness_mean:.4f}\")\n",
    "\n",
    "    # Group by psychiatric category\n",
    "    category_stats_faithfulness = merged_df.groupby('psychiatric_category').agg({\n",
    "        'Mean faithfulness for RAG': 'mean',\n",
    "    }).round(4)\n",
    "    category_stats_faithfulness = category_stats_faithfulness.sort_values('Mean faithfulness for RAG', ascending=False)\n",
    "    display(category_stats_faithfulness)\n",
    "\n",
    "    # EVALUATE RAG TRIAD - ANSWER RELEVANCY\n",
    "\n",
    "    with open(VANILLA_ANSWER_RELEVANCE, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    vanilla_answer_relevancy = pd.DataFrame(data)\n",
    "    vanilla_answer_relevancy = vanilla_answer_relevancy[[\"Modified Questions\",\n",
    "                                                         \"answer_relevancy for RAG run 1\",\n",
    "                                                    \"answer_relevancy for Vanilla run 2\",\n",
    "                                                    \"answer_relevancy for Vanilla run 3\",\n",
    "                                                    \"Mean answer_relevancy for Vanilla\"]]\n",
    "    \n",
    "    merged_df = pd.merge(merged_df, vanilla_answer_relevancy, on=\"Modified Questions\", how=\"inner\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with open(RAG_ANSWER_RELEVANCE, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    rag_answer_relevancy = pd.DataFrame(data)\n",
    "    rag_answer_relevancy = rag_answer_relevancy[[\"Modified Questions\",\n",
    "                                                 \"answer_relevancy for RAG run 1\",\n",
    "                                                    \"answer_relevancy for RAG run 2\",\n",
    "                                                    \"answer_relevancy for RAG run 3\",\n",
    "                                                    \"Mean answer_relevancy for RAG\"]]\n",
    "    \n",
    "\n",
    "    merged_df = pd.merge(merged_df, rag_answer_relevancy, on=\"Modified Questions\", how=\"inner\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    print(\"Answer Relevancy Results:\")\n",
    "    # Calculate overall means\n",
    "    vanilla_answer_relevancy_mean = merged_df['Mean answer_relevancy for Vanilla'].mean()\n",
    "    rag_answer_relevancy_mean = merged_df['Mean answer_relevancy for RAG'].mean()\n",
    "    print(f\"Overall Mean Answer Relevancy for Vanilla: {vanilla_answer_relevancy_mean:.4f}\")\n",
    "    print(f\"Overall Mean Relevancy for RAG: {relevancy_mean:.4f}\")\n",
    "\n",
    "    # Group by psychiatric category\n",
    "    category_stats_relevancy = merged_df.groupby('psychiatric_category').agg({\n",
    "        'Mean answer_relevancy for RAG': 'mean',\n",
    "        'Mean Answer Relevancy for Vanilla': 'mean'}).round(4)\n",
    "    category_stats_relevancy = category_stats_relevancy.sort_values('Mean answer_relevancy for RAG', ascending=False)\n",
    "    display(category_stats_relevancy)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "process_files(\"test_dataset_together_meta-llama_Llama-4-Scout-17B-16E-Instruct_top5_answered.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
