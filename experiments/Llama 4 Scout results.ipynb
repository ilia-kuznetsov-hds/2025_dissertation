{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e712d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135038a7",
   "metadata": {},
   "source": [
    "Evaluation setup:  \n",
    "* Embedding model - Text Embedding 004 (Google)  \n",
    "* LLM - Gemini 2.5 Flash  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd1067",
   "metadata": {},
   "source": [
    "## RAGAS Answer Correctness:\n",
    "\n",
    "The assessment of Answer Correctness involves measuring the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness.\n",
    "Answer correctness  is computed as the sum of factual correctness and the semantic similarity between the given answer and the ground truth.  \n",
    "\n",
    "Factual correctness is a metric that compares and evaluates the factual accuracy of the generated response with the reference. This metric is used to determine the extent to which the generated response aligns with the reference. The factual correctness score ranges from 0 to 1, with higher values indicating better performance. To measure the alignment between the response and the reference, the metric uses the LLM to first break down the response and reference into claims and then uses natural language inference to determine the factual overlap between the response and the reference. Factual overlap is quantified using precision, recall, and F1 score, which can be controlled using the mode parameter. By default, the mode is set to F1, you can change the mode to precision or recall by setting the mode parameter.\n",
    "\n",
    "Answer similarity is calculated by following steps:  \n",
    "Step 1: Vectorize the ground truth answer using the embedding model.  \n",
    "Step 2: Vectorize the generated answer using the same embedding model.  \n",
    "Step 3: Compute the cosine similarity between the two vectors.  \n",
    "        \n",
    "By default \"text-embedding-ada-002\" model is used. In that evaluation, we used Text Embedding 004 (Google).  \n",
    "\n",
    "Final score is created by taking a weighted average of the factual correctness (F1 score) and the semantic similarity. \n",
    "(By default, there is a 0.75 : 0.25 weighting.)   \n",
    "\n",
    "Total API Calls: 4  \n",
    "* 1 LLM call to produce the \"simple statements  \n",
    "* 1 LLM call to determine the true positives, false positives, and false negatives  \n",
    "* 1 embedding call to embed the context  \n",
    "* 1 embedding call to embed the AI answer  \n",
    "\n",
    "I feel like RAGAS use much more in terms of LLM calls. This is a source code:  \n",
    "https://github.com/explodinggradients/ragas/blob/main/ragas/src/ragas/metrics/_answer_correctness.py\n",
    "\n",
    "There is a compaint about RAGAS costing 700$ for 100 QA set:  \n",
    "https://www.reddit.com/r/LangChain/comments/1dbmqii/i_spent_700_on_evaluating_100_rag_qa_set_using/\n",
    "\n",
    "Because the generated results is non-deterministic, I run evaluation 3 times and use mean number. 370 questions x 3 times x 2 (vanilla and RAG) X 4 API calls = 8880 API calls total for one set of results.\n",
    "When I decided to play with Gemini 2.5 flash, ​I also got results for another metrics, in total dashboard shows 16000 API calls and 167 dollars spent.  \n",
    "Conclusion: I will stick with Gemini 2.0 flash model as main judge for the rest of the project\n",
    "\n",
    "Source:  \n",
    "https://docs.ragas.io/en/v0.1.21/concepts/metrics/answer_correctness.html  \n",
    "https://github.com/dkhundley/llm-rag-guide/blob/main/notebooks/ragas.ipynb  \n",
    "https://docs.ragas.io/en/stable/references/embeddings/#ragas.embeddings.embedding_factory  \n",
    "https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/factual_correctness/#factual-correctness  \n",
    "https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/semantic_similarity/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48695377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean Answer Correctness for Vanilla: 0.6058\n",
      "Overall Mean Answer Correctness for RAG: 0.5695\n",
      "Difference (RAG - Vanilla): -0.0364\n"
     ]
    }
   ],
   "source": [
    "file_name = \"test_dataset_together_meta-llama_Llama-4-Scout-17B-16E-Instruct_top5_answered.json\"\n",
    "base_name = file_name.replace('.json', '')\n",
    "\n",
    "    # Use forward slashes or os.path.join for better cross-platform compatibility\n",
    "VANILLA_ANSWER_CORRECTNESS = f\"{base_name}_vanilla_answer_correctness_evaluated.json\"\n",
    "RAG_ANSWER_CORRECTNESS = f\"{base_name}_rag_answer_correctness_evaluated.json\"\n",
    "RAG_ANSWER_SIMILARITY = f\"{base_name}_rag_answer_similarity_evaluated.json\"\n",
    "VANILLA_ANSWER_SIMILARITY = f\"{base_name}_vanilla_answer_similarity_evaluated.json\"\n",
    "RAG_ANSWER_RELEVANCY = f\"{base_name}_rag_answer_relevancy_evaluated.json\"\n",
    "RAG_FAITHFULNESS = f\"{base_name}_rag_faithfulness_evaluated.json\"\n",
    "\n",
    "with open(VANILLA_ANSWER_CORRECTNESS, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)   \n",
    "df_1 = pd.DataFrame(data)\n",
    "\n",
    "with open(RAG_ANSWER_CORRECTNESS, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f) \n",
    "df_2 = pd.DataFrame(data)\n",
    "df_2 = df_2[[\"Modified Questions\", \n",
    "                 \"Answer Correctness for RAG run 1\", \n",
    "                 \"Answer Correctness for RAG run 2\",\n",
    "                 \"Answer Correctness for RAG run 3\",\n",
    "                 \"Mean Answer Correctness for RAG\"]]\n",
    "    \n",
    "merged_df = pd.merge(df_1, df_2, on=\"Modified Questions\", how=\"inner\")\n",
    "# Calculate overall means\n",
    "vanilla_mean = merged_df['Mean Answer Correctness for vanilla'].mean()\n",
    "rag_mean = merged_df['Mean Answer Correctness for RAG'].mean()\n",
    "    \n",
    "print(f\"Overall Mean Answer Correctness for Vanilla: {vanilla_mean:.4f}\")\n",
    "print(f\"Overall Mean Answer Correctness for RAG: {rag_mean:.4f}\")\n",
    "print(f\"Difference (RAG - Vanilla): {rag_mean - vanilla_mean:.4f}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f77719a",
   "metadata": {},
   "source": [
    "Again, vanilla LLM response is evaluated higher than RAG-enhanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b0eef0",
   "metadata": {},
   "source": [
    "## RAGAS Answer semantic similarity  \n",
    "\n",
    "The metric is a part of RAGAS Answer Correctness metric.  \n",
    "\n",
    "The Answer Correctness's final score is created by taking a weighted average of the factual correctness (F1 score) and the semantic similarity. \n",
    "(By default, there is a 0.75 : 0.25 weighting.) \n",
    "\n",
    "This evaluation is based on the ground truth and the answer, with values falling within the range of 0 to 1. A higher score signifies a better alignment \n",
    "between the generated answer and the ground truth.  \n",
    "Step 1: Vectorize the ground truth answer using the specified embedding model.  \n",
    "Step 2: Vectorize the generated answer using the same embedding model.  \n",
    "Step 3: Compute the cosine similarity between the two vectors.  \n",
    "\n",
    "https://docs.ragas.io/en/v0.1.21/concepts/metrics/semantic_similarity.html\n",
    "https://github.com/dkhundley/llm-rag-guide/blob/main/notebooks/ragas.ipynb\n",
    "\n",
    "Total API Calls: 2\n",
    "* 1 embedding call to embed the ground truth\n",
    "* 1 embedding call to embed the AI answer  \n",
    "\n",
    "The metric is produced much faster (around 30 minutes for the whole set of results). It is 25% component of Answer Correctness metric.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32659d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean Answer Semantic Similarity for RAG: 0.8752\n",
      "Overall Mean Answer Semantic Similarity for Vanilla: 0.8797\n",
      "Difference (RAG - Vanilla): -0.0044\n",
      "                                                    Vanilla_Mean  RAG_Mean  \\\n",
      "psychiatric_category                                                         \n",
      "Bipolar Disorders                                         0.8346    0.8783   \n",
      "Somatic Disorders                                         0.8396    0.8762   \n",
      "Other Mental Disorders                                    0.8819    0.8709   \n",
      "Depressive Disorders                                      0.9060    0.8630   \n",
      "Anxiety Disorders                                         0.9342    0.8850   \n",
      "Dissociative Disorders                                       NaN    0.9323   \n",
      "Eating Disorders                                             NaN    0.8656   \n",
      "Obsessive-Compulsive Disorders                               NaN    0.8811   \n",
      "Personality Disorders                                        NaN    0.9062   \n",
      "Schizophrenia Spectrum and Other Psychotic Diso...           NaN    0.8752   \n",
      "Trauma and Stressor Related Disorders                        NaN    0.8849   \n",
      "\n",
      "                                                    Difference (RAG - Vanilla)  \n",
      "psychiatric_category                                                            \n",
      "Bipolar Disorders                                                       0.0437  \n",
      "Somatic Disorders                                                       0.0366  \n",
      "Other Mental Disorders                                                 -0.0110  \n",
      "Depressive Disorders                                                   -0.0430  \n",
      "Anxiety Disorders                                                      -0.0492  \n",
      "Dissociative Disorders                                                     NaN  \n",
      "Eating Disorders                                                           NaN  \n",
      "Obsessive-Compulsive Disorders                                             NaN  \n",
      "Personality Disorders                                                      NaN  \n",
      "Schizophrenia Spectrum and Other Psychotic Diso...                         NaN  \n",
      "Trauma and Stressor Related Disorders                                      NaN  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SET OF METRIC FOR VANILLA ISN'T COMPELETED, SO I WILL SKIP IT FOR NOW\n",
    "\n",
    "with open(RAG_ANSWER_SIMILARITY, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f) \n",
    "df_3 = pd.DataFrame(data)\n",
    "df_3 = df_3[[\"Modified Questions\", \n",
    "            \"Answer Semantic Similarity for rag\"]]\n",
    "\n",
    "\n",
    "with open(VANILLA_ANSWER_SIMILARITY, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f) \n",
    "df_4 = pd.DataFrame(data)\n",
    "df_4 = df_4[[\"Modified Questions\", \n",
    "            \"Answer Semantic Similarity for vanilla\"]]\n",
    "\n",
    "merged_df = pd.merge(merged_df, df_3, on=\"Modified Questions\", how=\"inner\")\n",
    "merged_df = pd.merge(merged_df, df_4, on=\"Modified Questions\", how=\"inner\")\n",
    "\n",
    "# Calculate overall means\n",
    "rag_similarity_mean = merged_df['Answer Semantic Similarity for rag'].mean()\n",
    "vanilla_similarity_mean = merged_df['Answer Semantic Similarity for vanilla'].mean()\n",
    "print(f\"Overall Mean Answer Semantic Similarity for RAG: {rag_similarity_mean:.4f}\")\n",
    "print(f\"Overall Mean Answer Semantic Similarity for Vanilla: {vanilla_similarity_mean:.4f}\")\n",
    "print(f\"Difference (RAG - Vanilla): {rag_similarity_mean - vanilla_similarity_mean:.4f}\")\n",
    "\n",
    "# Group by psychiatric category\n",
    "category_stats = merged_df.groupby('psychiatric_category').agg({\n",
    "        'Answer Semantic Similarity for vanilla': 'mean',\n",
    "        'Answer Semantic Similarity for rag': 'mean'}).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "category_stats.columns = ['Vanilla_Mean', 'RAG_Mean']\n",
    "    \n",
    "    # Add difference column\n",
    "category_stats['Difference (RAG - Vanilla)'] = (category_stats['RAG_Mean'] - \n",
    "                                                   category_stats['Vanilla_Mean']).round(4)\n",
    "    \n",
    "    # Sort by difference to see which categories benefit most from RAG\n",
    "category_stats = category_stats.sort_values('Difference (RAG - Vanilla)', ascending=False)\n",
    "    \n",
    "print(category_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1612bd7",
   "metadata": {},
   "source": [
    "# RAG Metrics  \n",
    "## RAGAS Faithfulness \n",
    "Faithfulness measures the factual consistency of the generated answer against the given context. It is calculated from the answer and the retrieved context. The answer is scaled to the (0, 1) range. The higher the better. \n",
    "The generated answer is regarded as faithful if all the claims made in the answer can be inferred from the given context.  \n",
    "At the first step, the generated answer is broken down into individual statements.     \n",
    "Then each of these claims is cross-checked with the given context to determine if it can be inferred from the context.    \n",
    "The final score is calculated by dividing the number of claims that can be inferred from the context by the total number of claims in the generated response.  \n",
    "\n",
    "\n",
    "https://docs.ragas.io/en/v0.1.21/concepts/metrics/faithfulness.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a435f6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean Faithfulness for RAG: 0.2678\n"
     ]
    }
   ],
   "source": [
    "with open(RAG_FAITHFULNESS, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f) \n",
    "df_3 = pd.DataFrame(data)\n",
    "df_3 = df_3[[\"Modified Questions\", \n",
    "                \"faithfulness for RAG run 1\",\n",
    "                \"faithfulness for RAG run 2\",\n",
    "                \"faithfulness for RAG run 3\",\n",
    "                \"Mean faithfulness for RAG\"]]\n",
    "    \n",
    "merged_df = pd.merge(merged_df, df_3, on=\"Modified Questions\", how=\"inner\")\n",
    "\n",
    "faithfulness_mean = merged_df['Mean faithfulness for RAG'].mean()\n",
    "print(f\"Overall Mean Faithfulness for RAG: {faithfulness_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f09b26",
   "metadata": {},
   "source": [
    "Only 26% of retrieved statements are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9cff34",
   "metadata": {},
   "source": [
    "## RAGAS Answer Relevancy\n",
    "\n",
    "\n",
    "Step 1: Reverse-engineer ‘n’ variants of the question from the generated answer using a Large Language Model (LLM)  \n",
    "Step 2: Calculate the mean cosine similarity between the generated questions and the actual question.\n",
    "\n",
    "\n",
    "\n",
    "https://docs.ragas.io/en/v0.1.21/concepts/metrics/answer_relevance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcab57c",
   "metadata": {},
   "source": [
    "## Final fucntion to generate report \n",
    "(work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e703c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in merged dataset:\n",
      "['Modified Questions', 'Reasonings', 'classification', 'psychiatry_reasoning', 'psychiatric_category', 'category_reasoning', 'category_confidence', 'Model', 'Provider', 'Generated Vanilla Answer', 'Generated RAG Answer', 'Retrieved Context', 'Top k Similarity', 'Top k Sparse', 'Sources Summary', 'Detailed Context', 'Answer Correctness for vanilla run 1', 'Answer Correctness for vanilla run 2', 'Answer Correctness for vanilla run 3', 'Mean Answer Correctness for vanilla', 'Evaluation Notes Answer Correctness for vanilla', 'Evaluation Model Answer Correctness for vanilla', 'Answer Correctness for RAG run 1', 'Answer Correctness for RAG run 2', 'Answer Correctness for RAG run 3', 'Mean Answer Correctness for RAG']\n",
      "\n",
      "Merged dataset shape: (369, 26)\n",
      "Overall Mean Answer Correctness for Vanilla: 0.6058\n",
      "Overall Mean Answer Correctness for RAG: 0.5695\n",
      "Difference (RAG - Vanilla): -0.0364\n",
      "Overall Mean Faithfulness for RAG: 0.2678\n",
      "                                                    Vanilla_Mean  RAG_Mean  \\\n",
      "psychiatric_category                                                         \n",
      "Personality Disorders                                     0.6966    0.6908   \n",
      "Anxiety Disorders                                         0.5663    0.5561   \n",
      "Eating Disorders                                          0.5493    0.5357   \n",
      "Somatic Disorders                                         0.6205    0.5937   \n",
      "Bipolar Disorders                                         0.5683    0.5397   \n",
      "Other Mental Disorders                                    0.6106    0.5820   \n",
      "Obsessive-Compulsive Disorders                            0.6064    0.5717   \n",
      "Depressive Disorders                                      0.5841    0.5492   \n",
      "Schizophrenia Spectrum and Other Psychotic Diso...        0.5955    0.5421   \n",
      "Trauma and Stressor Related Disorders                     0.6808    0.5785   \n",
      "Dissociative Disorders                                    0.8866    0.6991   \n",
      "\n",
      "                                                    Difference (RAG - Vanilla)  \n",
      "psychiatric_category                                                            \n",
      "Personality Disorders                                                  -0.0058  \n",
      "Anxiety Disorders                                                      -0.0102  \n",
      "Eating Disorders                                                       -0.0136  \n",
      "Somatic Disorders                                                      -0.0268  \n",
      "Bipolar Disorders                                                      -0.0286  \n",
      "Other Mental Disorders                                                 -0.0286  \n",
      "Obsessive-Compulsive Disorders                                         -0.0347  \n",
      "Depressive Disorders                                                   -0.0349  \n",
      "Schizophrenia Spectrum and Other Psychotic Diso...                     -0.0534  \n",
      "Trauma and Stressor Related Disorders                                  -0.1023  \n",
      "Dissociative Disorders                                                 -0.1875  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_files(file_name):\n",
    "    '''\n",
    "    The function will output full report for the set of results for one model.\n",
    "\n",
    "    '''\n",
    "    base_name = file_name.replace('.json', '')\n",
    "\n",
    "       # Use forward slashes or os.path.join for better cross-platform compatibility\n",
    "    VANILLA_ANSWER_CORRECTNESS = f\"{base_name}_vanilla_answer_correctness_evaluated.json\"\n",
    "    RAG_ANSWER_CORRECTNESS = f\"{base_name}_rag_answer_correctness_evaluated.json\"\n",
    "    RAG_ANSWER_SIMILARITY = f\"{base_name}_rag_answer_similarity_evaluated.json\"\n",
    "    VANILLA_ANSWER_SIMILARITY = f\"{base_name}_vanilla_answer_similarity_evaluated.json\"\n",
    "    RAG_ANSWER_RELEVANCY = f\"{base_name}_rag_answer_relevancy_evaluated.json\"\n",
    "    RAG_FAITHFULNESS = f\"{base_name}_rag_faithfulness_evaluated.json\"\n",
    "\n",
    "    with open(VANILLA_ANSWER_CORRECTNESS, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)   \n",
    "    df_1 = pd.DataFrame(data)\n",
    "\n",
    "    with open(RAG_ANSWER_CORRECTNESS, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f) \n",
    "    df_2 = pd.DataFrame(data)\n",
    "    df_2 = df_2[[\"Modified Questions\", \n",
    "                 \"Answer Correctness for RAG run 1\", \n",
    "                 \"Answer Correctness for RAG run 2\",\n",
    "                 \"Answer Correctness for RAG run 3\",\n",
    "                 \"Mean Answer Correctness for RAG\"]]\n",
    "    \n",
    "    merged_df = pd.merge(df_1, df_2, on=\"Modified Questions\", how=\"inner\")\n",
    "    \n",
    "    # Calculate overall means\n",
    "    vanilla_mean = merged_df['Mean Answer Correctness for vanilla'].mean()\n",
    "    rag_mean = merged_df['Mean Answer Correctness for RAG'].mean()\n",
    "    \n",
    "    print(f\"Overall Mean Answer Correctness for Vanilla: {vanilla_mean:.4f}\")\n",
    "    print(f\"Overall Mean Answer Correctness for RAG: {rag_mean:.4f}\")\n",
    "    print(f\"Difference (RAG - Vanilla): {rag_mean - vanilla_mean:.4f}\")\n",
    "\n",
    "    with open(RAG_FAITHFULNESS, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f) \n",
    "    df_3 = pd.DataFrame(data)\n",
    "    df_3 = df_3[[\"Modified Questions\", \n",
    "                \"faithfulness for RAG run 1\",\n",
    "                \"faithfulness for RAG run 2\",\n",
    "                \"faithfulness for RAG run 3\",\n",
    "                \"Mean faithfulness for RAG\"]]\n",
    "    \n",
    "    merged_df = pd.merge(merged_df, df_3, on=\"Modified Questions\", how=\"inner\")\n",
    "\n",
    "    faithfulness_mean = merged_df['Mean faithfulness for RAG'].mean()\n",
    "    print(f\"Overall Mean Faithfulness for RAG: {faithfulness_mean:.4f}\")\n",
    "\n",
    "\n",
    "    # Group by psychiatric category\n",
    "    category_stats = merged_df.groupby('psychiatric_category').agg({\n",
    "        'Mean Answer Correctness for vanilla': 'mean',\n",
    "        'Mean Answer Correctness for RAG': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "    category_stats.columns = ['Vanilla_Mean', 'RAG_Mean']\n",
    "    \n",
    "    # Add difference column\n",
    "    category_stats['Difference (RAG - Vanilla)'] = (category_stats['RAG_Mean'] - \n",
    "                                                   category_stats['Vanilla_Mean']).round(4)\n",
    "    \n",
    "    # Sort by difference to see which categories benefit most from RAG\n",
    "    category_stats = category_stats.sort_values('Difference (RAG - Vanilla)', ascending=False)\n",
    "    \n",
    "    print(category_stats)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "process_files(\"test_dataset_together_meta-llama_Llama-4-Scout-17B-16E-Instruct_top5_answered.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
