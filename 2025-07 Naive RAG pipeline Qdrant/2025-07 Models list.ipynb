{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3247d0",
   "metadata": {},
   "source": [
    "Short description of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aee937",
   "metadata": {},
   "source": [
    "### DeepSeek V3\n",
    "\n",
    "DeepSeek-V3 is a Mixture-of-Experts (MoE) language model with 671 billion total parameters, activating 37 billion parameters for each token. Its post-training involved an knowledge distillation methodology from the DeepSeek R1 reasoning model.  \n",
    "Source:  \n",
    "[HuggingFace Model Card](https://huggingface.co/deepseek-ai/DeepSeek-V3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5eba92",
   "metadata": {},
   "source": [
    "### ExaOne 3.5 32B  \n",
    "The EXAONE 3.5 language models are based on the decoder-only Transformer architecture. ExaOne 3.5 32B is the instruction-tuned 32B language model with the following features:\n",
    "* Number of Parameters (without embeddings): 30.95B  \n",
    "* Number of Layers: 64  \n",
    "* Number of Attention Heads: GQA with 40 Q-heads and 8 KV-heads  \n",
    "* Vocab Size: 102,400 (?)   \n",
    "* Context Length: 32,768 tokens  \n",
    "\n",
    "Source:   \n",
    "[Technical Report](https://arxiv.org/pdf/2412.04862)  \n",
    "[HuggingFace model card](https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-32B-Instruct)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb5ea2",
   "metadata": {},
   "source": [
    "### Meta Llama 4 Scout (17B x 16E)  \n",
    "The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality. Llama Scout is a 17 billion parameter model with 16 experts.  \n",
    "* Parameters: 17B (Activated) 109B (Total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5352923b",
   "metadata": {},
   "source": [
    "### Meta Llama 4 Maverick (17B x 128E)  \n",
    "Llama 4 series, Llama 4 Maverick is a 17 billion parameter model with 128 experts.  \n",
    "\n",
    "* Parameters: 17B (Activated) 400B (Total)  \n",
    "\n",
    "Source:  \n",
    "[HuggingFace Modle Card](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971c5f4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
