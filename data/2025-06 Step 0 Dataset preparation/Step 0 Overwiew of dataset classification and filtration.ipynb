{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8debd8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0681d0a",
   "metadata": {},
   "source": [
    "We use MedQA-Open dataset from the paper \"Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering\"  \n",
    "The dataset available at (ancillary files setion):  \n",
    "https://arxiv.org/abs/2403.04890  \n",
    "\n",
    "The author of the paper used the USMLE-MedQA dataset (Jin et al., 2021), a medical exam dataset that consists of questions   \n",
    "sourced from professional medical board exams in the USA.  \n",
    "\n",
    "\n",
    "The authors used the MedQA dataset (Zhang et al., 2018) is a publicly available collection of complex medical questions  \n",
    "with multiple choices based on the United States medical license exams. To emulate real-world medical scenarios,   \n",
    "they convert these multiple-choice questions into open-ended questions by (1) removing the multiple-choice options and   \n",
    "(2) re-pharsing the question to be open-ended using LLM, creating MedQA-Open.   \n",
    "\n",
    "The dataset contains more than 10k questions related to different medical fields, including psychiatry.   \n",
    "The following script uses LLM to classify questions as psychiatry-related or not psychiatry-related.  \n",
    "It is a screening step that uses fast cost-efficient LLM to filter out non-psychiatry questions   \n",
    "before using more expensive LLMs for detailed analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57e3c7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10178 rows in the original dataset\n"
     ]
    }
   ],
   "source": [
    "ORIGINAL_DATASET_PATH = \"MedQA_open_dataset.xlsx\"\n",
    "original_df = pd.read_excel(ORIGINAL_DATASET_PATH)\n",
    "print(len(original_df), \"rows in the original dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e941326",
   "metadata": {},
   "source": [
    "I used used LLM (gemini 2.0 flash model) with rather simple prompt to screen for psychiatry-related questions. Google was used as provider of LLM since it gives generous Free-tier limit for experimenting with LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc4a77c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\n",
    "answer = \"\" # Just placeholders\n",
    "\n",
    "prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    Is this question related to psychiatry? Respond with only 'psychiatry' or 'non-psychiatry'.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd0f30",
   "metadata": {},
   "source": [
    "If either question or answer column was empty, we would classify question as invalid. We don't provide LLM with empty query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05cb58e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10178 rows in the screened dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "classification\n",
       "non-psychiatry    8780\n",
       "psychiatry         886\n",
       "invalid            512\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCREENED_QUESTIONS_PATH = \"MedQA_open_dataset_classified.xlsx\"\n",
    "screened_df = pd.read_excel(SCREENED_QUESTIONS_PATH)\n",
    "print(len(screened_df), \"rows in the screened dataset\")\n",
    "screened_df[\"classification\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb7646",
   "metadata": {},
   "source": [
    "After initial screening, only 886 question were considered as psychiatry-related. However, the prompt was weak and model was selected due to speed rather than precision. A the next step we verified that question actually asks about psychiatry, not just mentions psychiatric concepts in the different cases vignettes. To do so, we prompted a newer model gemini 2.5 flash with the folllowing prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1f4313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "    Act as an experienced clinical psychiatrist and medical educator. \n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Evaluate provided question and reasoning based on the following criteria:\n",
    "\n",
    "    CLINICAL PSYCHIATRY FOCUS: Is this question primarily testing knowledge of clinical psychiatry, mental health disorders, psychiatric treatments, or psychological concepts? \n",
    "    - Questions that merely mention mental health terms but primarily test other medical knowledge (like diabetes, cardiology, etc.) should be excluded\n",
    "    - Questions should focus on psychiatric diagnosis, treatment, symptoms, or mental health concepts as the main learning objective\n",
    "\n",
    "    Provide your response in the following JSON format:\n",
    "    {{\n",
    "        \"classification\": \"INCLUDE\" or \"EXCLUDE\",\n",
    "        \"reasoning\": \"Brief explanation of why you included or excluded this question\"\n",
    "    }}\n",
    "\n",
    "    Classification options:\n",
    "    - \"INCLUDE\" if the question is primarily focused on clinical psychiatry AND the reasoning is useful\n",
    "    - \"EXCLUDE\" if the first criteria fail\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0e256c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification value counts:\n",
      "psychiatry_classification\n",
      "include    737\n",
      "exclude    147\n",
      "re-do        2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "with open(\"MedQA_open_dataset_classified_psychiatry_evaluation.json\", 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "verified_df = pd.DataFrame(data)\n",
    "print(\"Classification value counts:\")\n",
    "print(verified_df[\"psychiatry_classification\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946bf462",
   "metadata": {},
   "source": [
    "Overall, we have 737 psychiatry-related questions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
